"""
LaMP (Language Model Personalization) Benchmark í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ v2
Provider íŒ¨í„´ì„ ì‚¬ìš©í•˜ì—¬ ë¦¬íŒ©í† ë§ëœ ë²„ì „

LaMP ë²¤ì¹˜ë§ˆí¬ íƒœìŠ¤í¬:
- LaMP-1: Personalized Citation Identification (ë…¼ë¬¸ ì¸ìš© ì˜ˆì¸¡)
- LaMP-2: Personalized Movie Tagging (ì˜í™” íƒœê¹…)
- LaMP-3: Personalized Product Rating (ì œí’ˆ í‰ì  ì˜ˆì¸¡)
- LaMP-4: Personalized News Headline Generation (ë‰´ìŠ¤ í—¤ë“œë¼ì¸ ìƒì„±)
- LaMP-5: Personalized Scholarly Title Generation (ë…¼ë¬¸ ì œëª© ìƒì„±)
- LaMP-6: Personalized Email Subject Generation (ì´ë©”ì¼ ì œëª© ìƒì„±)
- LaMP-7: Personalized Tweet Paraphrasing (íŠ¸ìœ— íŒ¨ëŸ¬í”„ë ˆì´ì§•)

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” Provider íŒ¨í„´ì„ ì‚¬ìš©í•˜ì—¬ ê°œì¸í™” ì‹œìŠ¤í…œì„ í‰ê°€í•©ë‹ˆë‹¤.
"""

import asyncio
import json
import os
import sys
import time
import math
import random
from datetime import datetime, timedelta
from typing import List, Dict, Any, Tuple, Optional
from dataclasses import dataclass
from collections import defaultdict
import uuid

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from dotenv import load_dotenv
load_dotenv()

# Provider imports
from config import settings
from providers.embedding import get_embedding_provider
from providers.vectordb import get_vectordb_provider
from providers.reranker import get_reranker_provider
from providers.llm import get_llm_provider

# Search enhancements (Query Expansion, RRF)
from services.search_enhancements import SearchEnhancer, RRFusion, get_search_enhancer

try:
    import numpy as np
except ImportError:
    print("numpy ì„¤ì¹˜ í•„ìš”: pip install numpy")
    sys.exit(1)


@dataclass
class LaMP_Profile:
    """LaMP ì‚¬ìš©ì í”„ë¡œí•„ ë°ì´í„°"""
    user_id: str
    profile_items: List[Dict[str, Any]]
    task_type: str


@dataclass
class LaMP_Query:
    """LaMP ì¿¼ë¦¬ ë°ì´í„°"""
    query_id: str
    user_id: str
    query_text: str
    task_type: str
    ground_truth: str
    relevant_profile_ids: List[str]


class BM25:
    """BM25 ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜"""

    def __init__(self, k1: float = 1.5, b: float = 0.75):
        self.k1 = k1
        self.b = b
        self.doc_freqs = {}
        self.doc_lens = []
        self.avg_doc_len = 0
        self.corpus_size = 0
        self.documents = []
        self.tokenized_docs = []

    def tokenize(self, text: str) -> List[str]:
        import re
        text = text.lower()
        tokens = re.findall(r'\b\w+\b', text)
        return tokens

    def fit(self, documents: List[str]):
        self.documents = documents
        self.corpus_size = len(documents)
        self.tokenized_docs = [self.tokenize(doc) for doc in documents]
        self.doc_lens = [len(doc) for doc in self.tokenized_docs]
        self.avg_doc_len = sum(self.doc_lens) / self.corpus_size if self.corpus_size > 0 else 0
        self.doc_freqs = defaultdict(int)
        for doc in self.tokenized_docs:
            for term in set(doc):
                self.doc_freqs[term] += 1

    def get_scores(self, query: str) -> List[float]:
        query_tokens = self.tokenize(query)
        scores = []
        for idx, doc in enumerate(self.tokenized_docs):
            score = 0.0
            doc_len = self.doc_lens[idx]
            term_freqs = defaultdict(int)
            for term in doc:
                term_freqs[term] += 1
            for term in query_tokens:
                if term not in term_freqs:
                    continue
                tf = term_freqs[term]
                df = self.doc_freqs.get(term, 0)
                if df == 0:
                    continue
                idf = math.log((self.corpus_size - df + 0.5) / (df + 0.5) + 1)
                tf_norm = (tf * (self.k1 + 1)) / (tf + self.k1 * (1 - self.b + self.b * doc_len / self.avg_doc_len))
                score += idf * tf_norm
            scores.append(score)
        return scores

    def search(self, query: str, top_k: int = 10) -> List[Tuple[int, float]]:
        scores = self.get_scores(query)
        indexed_scores = [(i, score) for i, score in enumerate(scores)]
        indexed_scores.sort(key=lambda x: x[1], reverse=True)
        return indexed_scores[:top_k]


class LaMP_Benchmark_V2:
    """Provider íŒ¨í„´ì„ ì‚¬ìš©í•˜ëŠ” LaMP ë²¤ì¹˜ë§ˆí¬"""

    def __init__(self):
        self.embedding_provider = None
        self.vectordb_provider = None
        self.reranker_provider = None
        self.llm_provider = None
        self.search_enhancer = None
        self.collection_name = "lamp_benchmark_v2"

        # BM25 ì¸ë±ìŠ¤
        self.bm25 = BM25()
        self.documents = []
        self.document_ids = []

        # RRF Fusion
        self.rrf = RRFusion(k=60)

        self._initialized = False

    async def initialize(self):
        """Provider ì´ˆê¸°í™”"""
        if self._initialized:
            return

        print("ğŸ”§ Provider ì´ˆê¸°í™” ì¤‘...")

        # Embedding Provider
        try:
            self.embedding_provider = get_embedding_provider()
            print(f"   âœ… Embedding: {settings.providers.embedding_provider} ({settings.providers.embedding_model})")
        except Exception as e:
            print(f"   âŒ Embedding ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

        # VectorDB Provider
        try:
            self.vectordb_provider = get_vectordb_provider()
            print(f"   âœ… VectorDB: {settings.providers.vectordb_provider}")
        except Exception as e:
            print(f"   âŒ VectorDB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

        # Reranker Provider
        try:
            self.reranker_provider = get_reranker_provider()
            print(f"   âœ… Reranker: {settings.providers.reranker_provider}")
        except Exception as e:
            print(f"   âš ï¸ Reranker ì´ˆê¸°í™” ì‹¤íŒ¨ (ë¹„í™œì„±í™”): {e}")
            self.reranker_provider = None

        # LLM Provider (Query Expansionìš©) - gpt5-mini ì‚¬ìš©
        try:
            self.llm_provider = get_llm_provider(model="gpt5-mini")
            self.search_enhancer = get_search_enhancer(self.llm_provider)
            print(f"   âœ… Search Enhancer: Query Expansion + RRF Fusion (gpt5-mini)")
        except Exception as e:
            print(f"   âš ï¸ Search Enhancer ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.search_enhancer = None

        self._initialized = True
        print()

    def generate_lamp_data(self, num_users: int = 10, items_per_user: int = 20, seed: int = 42) -> Tuple[List[LaMP_Profile], List[LaMP_Query]]:
        """LaMP ìŠ¤íƒ€ì¼ ë°ì´í„° ìƒì„± (seedë¡œ ì¬í˜„ ê°€ëŠ¥)"""
        # ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•´ seed ì„¤ì •
        random.seed(seed)
        np.random.seed(seed)

        profiles = []
        queries = []

        # ì‚¬ìš©ì í˜ë¥´ì†Œë‚˜ ì •ì˜
        user_personas = [
            {"name": "tech_enthusiast", "interests": ["AI", "programming", "gadgets"], "style": "technical",
             "products": ["laptop", "smartphone", "headphones"], "rating_bias": 3.5},
            {"name": "casual_user", "interests": ["movies", "music", "travel"], "style": "casual",
             "products": ["camera", "speakers", "travel gear"], "rating_bias": 4.2},
            {"name": "professional", "interests": ["productivity", "business"], "style": "formal",
             "products": ["office equipment", "software", "books"], "rating_bias": 3.8},
            {"name": "creative", "interests": ["art", "design", "photography"], "style": "expressive",
             "products": ["graphics tablet", "camera", "software"], "rating_bias": 4.5},
            {"name": "student", "interests": ["studying", "entertainment"], "style": "informal",
             "products": ["textbooks", "laptop", "headphones"], "rating_bias": 3.5},
            {"name": "health_focused", "interests": ["fitness", "nutrition"], "style": "motivational",
             "products": ["fitness tracker", "supplements", "workout gear"], "rating_bias": 4.0},
            {"name": "minimalist", "interests": ["simple living", "quality"], "style": "concise",
             "products": ["essential items", "quality tools"], "rating_bias": 4.0},
            {"name": "gamer", "interests": ["video games", "esports"], "style": "enthusiastic",
             "products": ["gaming PC", "monitor", "keyboard"], "rating_bias": 4.3},
            {"name": "parent", "interests": ["family", "children", "home"], "style": "practical",
             "products": ["toys", "educational items", "appliances"], "rating_bias": 3.9},
            {"name": "senior", "interests": ["simplicity", "health"], "style": "clear",
             "products": ["easy devices", "health monitors"], "rating_bias": 4.1},
        ]

        products_db = {
            "laptop": ["MacBook Pro M3", "Dell XPS 15", "ThinkPad X1", "ASUS ROG", "Surface Laptop"],
            "smartphone": ["iPhone 15 Pro", "Galaxy S24 Ultra", "Pixel 8 Pro", "OnePlus 12"],
            "headphones": ["Sony WH-1000XM5", "AirPods Pro 2", "Bose QC Ultra", "Sennheiser Momentum"],
            "camera": ["Sony A7 IV", "Canon EOS R6", "Nikon Z6", "Fujifilm X-T5"],
            "gaming PC": ["ROG Strix", "Alienware Aurora", "HP Omen", "MSI Trident"],
            "monitor": ["LG UltraFine", "Dell UltraSharp", "ASUS ProArt", "Samsung Odyssey"],
            "keyboard": ["Keychron Q1", "Logitech MX Keys", "HHKB Professional", "Corsair K100"],
            "fitness tracker": ["Apple Watch Ultra", "Garmin Fenix", "Fitbit Sense", "Samsung Galaxy Watch"],
        }

        for user_idx in range(min(num_users, len(user_personas))):
            persona = user_personas[user_idx]
            user_id = f"user_{user_idx + 1}"
            profile_items = []

            for item_idx in range(items_per_user):
                category = random.choice(persona["products"])
                products = products_db.get(category, products_db["laptop"])
                product = random.choice(products)

                # í‰ì  (í˜ë¥´ì†Œë‚˜ í¸í–¥ ë°˜ì˜)
                rating = min(5, max(1, int(random.gauss(persona["rating_bias"], 0.8))))

                # ë¦¬ë·° ìƒì„±
                review = self._generate_review(product, rating, persona["style"])

                profile_items.append({
                    "id": f"{user_id}_item_{item_idx}",
                    "product": product,
                    "category": category,
                    "rating": rating,
                    "review": review,
                    "date": (datetime.now() - timedelta(days=random.randint(1, 365))).isoformat(),
                })

            profiles.append(LaMP_Profile(user_id=user_id, profile_items=profile_items, task_type="LaMP-3"))

            # ì¿¼ë¦¬ ìƒì„±
            for q_idx in range(3):
                query_category = random.choice(persona["products"])
                products = products_db.get(query_category, products_db["laptop"])
                query_product = random.choice(products)

                relevant_ids = [item["id"] for item in profile_items if item["category"] == query_category][:5]
                related_ratings = [item["rating"] for item in profile_items if item["category"] == query_category]
                expected_rating = round(sum(related_ratings) / len(related_ratings)) if related_ratings else 4

                queries.append(LaMP_Query(
                    query_id=f"{user_id}_query_{q_idx}",
                    user_id=user_id,
                    query_text=f"{query_product}ì— ëŒ€í•´ ì´ ì‚¬ìš©ìëŠ” ì–´ë–¤ í‰ì ì„ ì¤„ê¹Œìš”?",
                    task_type="LaMP-3",
                    ground_truth=str(expected_rating),
                    relevant_profile_ids=relevant_ids
                ))

        return profiles, queries

    def _generate_review(self, product: str, rating: int, style: str) -> str:
        """ë¦¬ë·° ìƒì„±"""
        templates = {
            "technical": f"{product} ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë§Œì¡±ìŠ¤ëŸ½ìŠµë‹ˆë‹¤. {rating}/5ì .",
            "casual": f"{product} ì§„ì§œ ì¢‹ì•„ìš”! {rating}ì  ë“œë ¤ìš”~",
            "formal": f"{product}ì— ëŒ€í•œ í‰ê°€: ì „ë°˜ì ìœ¼ë¡œ {'ìš°ìˆ˜' if rating >= 4 else 'ë³´í†µ'}í•©ë‹ˆë‹¤. {rating}/5",
            "expressive": f"ì™€! {product} ì™„ì „ ì‚¬ë‘í•´ìš” ğŸ’• {rating}ì !",
            "informal": f"ã…‹ã…‹ {product} {'ê°œì´ë“' if rating >= 4 else 'ê·¸ì €ê·¸ëŸ¼'} {rating}ì ",
            "motivational": f"{product}ë¡œ ëª©í‘œ ë‹¬ì„± ì¤‘! {rating}/5ì ",
            "concise": f"{product}: {rating}/5",
            "enthusiastic": f"{product} ìµœê³ !!! {rating}ì !!!",
            "practical": f"{product} {'ì¶”ì²œ' if rating >= 4 else 'ë³´í†µ'}: {rating}/5",
            "clear": f"{product} - {rating}ì , {'ì¢‹ìŒ' if rating >= 4 else 'ë³´í†µ'}",
        }
        return templates.get(style, templates["casual"])

    async def setup_collection(self):
        """ë²¡í„° DB ì»¬ë ‰ì…˜ ì„¤ì •"""
        dimension = self.embedding_provider.dimension

        # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ í›„ ì¬ìƒì„±
        try:
            await self.vectordb_provider.delete_collection(self.collection_name)
        except:
            pass

        await self.vectordb_provider.create_collection(
            collection_name=self.collection_name,
            dimension=dimension
        )
        print(f"âœ… Collection '{self.collection_name}' ìƒì„± ì™„ë£Œ (dim={dimension})")

    async def index_profiles(self, profiles: List[LaMP_Profile]):
        """í”„ë¡œí•„ ì¸ë±ì‹±"""
        vectors = []
        self.documents = []
        self.document_ids = []

        print("ğŸ“Š í”„ë¡œí•„ ì„ë² ë”© ìƒì„± ì¤‘...")
        all_texts = []
        all_payloads = []

        for profile in profiles:
            for item in profile.profile_items:
                text = f"ì œí’ˆ: {item['product']} | ì¹´í…Œê³ ë¦¬: {item['category']} | í‰ì : {item['rating']}/5 | ë¦¬ë·°: {item['review']}"
                all_texts.append(text)
                all_payloads.append({
                    "item_id": item["id"],
                    "user_id": profile.user_id,
                    "product": item["product"],
                    "category": item["category"],
                    "rating": item["rating"],
                    "review": item["review"],
                    "text": text,
                })
                self.documents.append(text)
                self.document_ids.append(item["id"])

        # ë°°ì¹˜ ì„ë² ë”©
        batch_size = 50
        all_embeddings = []
        for i in range(0, len(all_texts), batch_size):
            batch = all_texts[i:i+batch_size]
            embeddings = await self.embedding_provider.embed(batch)
            all_embeddings.extend(embeddings)
            print(f"   ì„ë² ë”©: {min(i+batch_size, len(all_texts))}/{len(all_texts)}")

        # ë²¡í„° êµ¬ì„±
        for idx, (embedding, payload) in enumerate(zip(all_embeddings, all_payloads)):
            vectors.append({
                "id": str(uuid.uuid4()),
                "vector": embedding,
                "payload": payload
            })

        # ì—…ë¡œë“œ
        await self.vectordb_provider.upsert(self.collection_name, vectors)

        # BM25 ì¸ë±ìŠ¤
        self.bm25.fit(self.documents)

        print(f"âœ… {len(vectors)}ê°œ í”„ë¡œí•„ ì¸ë±ì‹± ì™„ë£Œ")

    async def search_2stage(self, query: str, user_id: str, top_k: int = 5) -> Tuple[List[Dict], float]:
        """2-stage: Vector Search â†’ Reranking"""
        start_time = time.time()

        # 1. Vector Search
        query_embedding = (await self.embedding_provider.embed([query]))[0]

        results = await self.vectordb_provider.search(
            collection_name=self.collection_name,
            query_vector=query_embedding,
            top_k=top_k * 3,
            filter_conditions={"user_id": user_id}
        )

        candidates = [
            {
                "item_id": r.payload["item_id"],
                "text": r.payload["text"],
                "score": r.score,
                "product": r.payload["product"],
                "category": r.payload["category"],
                "rating": r.payload["rating"]
            }
            for r in results
        ]

        # 2. Reranking
        if self.reranker_provider and candidates:
            try:
                rerank_results = await self.reranker_provider.rerank(
                    query=query,
                    documents=[c["text"] for c in candidates],
                    top_k=top_k
                )
                reranked = []
                for r in rerank_results:
                    candidate = candidates[r.index]
                    candidate["rerank_score"] = r.score
                    reranked.append(candidate)
                candidates = reranked
            except Exception as e:
                print(f"   Reranking ì˜¤ë¥˜: {e}")
                candidates = candidates[:top_k]
        else:
            candidates = candidates[:top_k]

        latency = (time.time() - start_time) * 1000
        return candidates, latency

    async def search_3stage(self, query: str, user_id: str, top_k: int = 5) -> Tuple[List[Dict], float]:
        """3-stage: Vector Search â†’ BM25 Hybrid â†’ Reranking"""
        start_time = time.time()

        # 1. Vector Search
        query_embedding = (await self.embedding_provider.embed([query]))[0]

        results = await self.vectordb_provider.search(
            collection_name=self.collection_name,
            query_vector=query_embedding,
            top_k=top_k * 3,
            filter_conditions={"user_id": user_id}
        )

        vector_candidates = {
            r.payload["item_id"]: {
                "item_id": r.payload["item_id"],
                "text": r.payload["text"],
                "vector_score": r.score,
                "product": r.payload["product"],
                "category": r.payload["category"],
                "rating": r.payload["rating"]
            }
            for r in results
        }

        # 2. BM25 Hybrid
        bm25_results = self.bm25.search(query, top_k=top_k * 3)
        user_doc_indices = [i for i, doc_id in enumerate(self.document_ids) if doc_id.startswith(user_id)]
        bm25_filtered = [(idx, score) for idx, score in bm25_results if idx in user_doc_indices]

        if bm25_filtered:
            max_bm25 = max(score for _, score in bm25_filtered) or 1
            for idx, bm25_score in bm25_filtered:
                item_id = self.document_ids[idx]
                if item_id in vector_candidates:
                    vector_candidates[item_id]["bm25_score"] = bm25_score / max_bm25

        # Hybrid score
        for item_id, candidate in vector_candidates.items():
            vector_score = candidate.get("vector_score", 0)
            bm25_score = candidate.get("bm25_score", 0)
            candidate["hybrid_score"] = 0.7 * vector_score + 0.3 * bm25_score

        candidates = sorted(vector_candidates.values(), key=lambda x: x.get("hybrid_score", 0), reverse=True)[:top_k * 2]

        # 3. Reranking
        if self.reranker_provider and candidates:
            try:
                rerank_results = await self.reranker_provider.rerank(
                    query=query,
                    documents=[c["text"] for c in candidates],
                    top_k=top_k
                )
                reranked = []
                for r in rerank_results:
                    candidate = candidates[r.index]
                    candidate["rerank_score"] = r.score
                    reranked.append(candidate)
                candidates = reranked
            except Exception as e:
                print(f"   Reranking ì˜¤ë¥˜: {e}")
                candidates = candidates[:top_k]
        else:
            candidates = candidates[:top_k]

        latency = (time.time() - start_time) * 1000
        return candidates, latency

    async def search_rrf_hybrid(self, query: str, user_id: str, top_k: int = 5) -> Tuple[List[Dict], float]:
        """RRF Hybrid: Vector + BM25 â†’ RRF Fusion â†’ Reranking"""
        start_time = time.time()

        # 1. Vector Search
        query_embedding = (await self.embedding_provider.embed([query]))[0]

        results = await self.vectordb_provider.search(
            collection_name=self.collection_name,
            query_vector=query_embedding,
            top_k=top_k * 3,
            filter_conditions={"user_id": user_id}
        )

        vector_candidates = [
            {
                "id": r.payload["item_id"],
                "item_id": r.payload["item_id"],
                "text": r.payload["text"],
                "vector_score": r.score,
                "product": r.payload["product"],
                "category": r.payload["category"],
                "rating": r.payload["rating"]
            }
            for r in results
        ]

        # 2. BM25 Search
        bm25_results = self.bm25.search(query, top_k=top_k * 3)
        user_doc_indices = [i for i, doc_id in enumerate(self.document_ids) if doc_id.startswith(user_id)]
        bm25_filtered = [(idx, score) for idx, score in bm25_results if idx in user_doc_indices]

        bm25_candidates = []
        for idx, bm25_score in bm25_filtered:
            item_id = self.document_ids[idx]
            bm25_candidates.append({
                "id": item_id,
                "item_id": item_id,
                "text": self.documents[idx],
                "bm25_score": bm25_score
            })

        # 3. RRF Fusion
        candidates = self.rrf.fuse_with_scores(
            vector_candidates, bm25_candidates, id_key="id", top_k=top_k * 2
        )

        # 4. Reranking
        if self.reranker_provider and candidates:
            try:
                rerank_results = await self.reranker_provider.rerank(
                    query=query,
                    documents=[c["text"] for c in candidates],
                    top_k=top_k
                )
                reranked = []
                for r in rerank_results:
                    candidate = candidates[r.index]
                    candidate["rerank_score"] = r.score
                    reranked.append(candidate)
                candidates = reranked
            except Exception as e:
                print(f"   Reranking ì˜¤ë¥˜: {e}")
                candidates = candidates[:top_k]
        else:
            candidates = candidates[:top_k]

        latency = (time.time() - start_time) * 1000
        return candidates, latency

    async def search_4stage(self, query: str, user_id: str, top_k: int = 5) -> Tuple[List[Dict], float]:
        """4-stage: Query Expansion â†’ Vector Search â†’ RRF Fusion â†’ Reranking"""
        start_time = time.time()

        # 0. Query Expansion
        queries_to_search = [query]
        if self.search_enhancer:
            try:
                expanded = await self.search_enhancer.expand_query(query)
                queries_to_search = expanded.get_all_queries()
            except Exception as e:
                print(f"   Query expansion ì˜¤ë¥˜: {e}")

        # 1. Vector Search for each expanded query
        all_vector_results = []
        for q in queries_to_search:
            q_embedding = (await self.embedding_provider.embed([q]))[0]
            results = await self.vectordb_provider.search(
                collection_name=self.collection_name,
                query_vector=q_embedding,
                top_k=top_k * 2,
                filter_conditions={"user_id": user_id}
            )
            vector_candidates = [
                {
                    "id": r.payload["item_id"],
                    "item_id": r.payload["item_id"],
                    "text": r.payload["text"],
                    "vector_score": r.score,
                    "product": r.payload["product"],
                    "category": r.payload["category"],
                    "rating": r.payload["rating"]
                }
                for r in results
            ]
            if vector_candidates:
                all_vector_results.append(vector_candidates)

        # 2. RRF Fusion across expanded queries
        if len(all_vector_results) > 1:
            candidates = self.rrf.fuse(all_vector_results, id_key="id", top_k=top_k * 2)
        elif all_vector_results:
            candidates = all_vector_results[0][:top_k * 2]
        else:
            return [], (time.time() - start_time) * 1000

        # 3. BM25 + RRF for hybrid
        bm25_results = self.bm25.search(query, top_k=top_k * 3)
        user_doc_indices = [i for i, doc_id in enumerate(self.document_ids) if doc_id.startswith(user_id)]
        bm25_filtered = [(idx, score) for idx, score in bm25_results if idx in user_doc_indices]

        bm25_candidates = []
        for idx, bm25_score in bm25_filtered:
            item_id = self.document_ids[idx]
            bm25_candidates.append({
                "id": item_id,
                "item_id": item_id,
                "text": self.documents[idx],
                "bm25_score": bm25_score
            })

        if bm25_candidates:
            candidates = self.rrf.fuse_with_scores(
                candidates, bm25_candidates, id_key="id", top_k=top_k * 2
            )

        # 4. Reranking
        if self.reranker_provider and candidates:
            try:
                rerank_results = await self.reranker_provider.rerank(
                    query=query,
                    documents=[c["text"] for c in candidates],
                    top_k=top_k
                )
                reranked = []
                for r in rerank_results:
                    candidate = candidates[r.index]
                    candidate["rerank_score"] = r.score
                    reranked.append(candidate)
                candidates = reranked
            except Exception as e:
                print(f"   Reranking ì˜¤ë¥˜: {e}")
                candidates = candidates[:top_k]
        else:
            candidates = candidates[:top_k]

        latency = (time.time() - start_time) * 1000
        return candidates, latency

    async def evaluate(self, queries: List[LaMP_Query], include_enhanced: bool = True) -> Dict[str, Any]:
        """ë²¤ì¹˜ë§ˆí¬ í‰ê°€"""
        results = {
            "2-stage": {"hits": [], "mrr": [], "ndcg": [], "precision": [], "recall": [], "latency": []},
            "3-stage": {"hits": [], "mrr": [], "ndcg": [], "precision": [], "recall": [], "latency": []},
            "rrf-hybrid": {"hits": [], "mrr": [], "ndcg": [], "precision": [], "recall": [], "latency": []},
            "4-stage": {"hits": [], "mrr": [], "ndcg": [], "precision": [], "recall": [], "latency": []}
        }

        total = len(queries)

        for idx, query in enumerate(queries):
            print(f"\rí‰ê°€ ì¤‘... {idx+1}/{total}", end="", flush=True)
            relevant_ids = set(query.relevant_profile_ids)

            # 2-stage
            results_2, latency_2 = await self.search_2stage(query.query_text, query.user_id)
            retrieved_2 = [r["item_id"] for r in results_2]
            metrics_2 = self._calculate_metrics(retrieved_2, relevant_ids)
            for key, value in metrics_2.items():
                results["2-stage"][key].append(value)
            results["2-stage"]["latency"].append(latency_2)

            # 3-stage
            results_3, latency_3 = await self.search_3stage(query.query_text, query.user_id)
            retrieved_3 = [r["item_id"] for r in results_3]
            metrics_3 = self._calculate_metrics(retrieved_3, relevant_ids)
            for key, value in metrics_3.items():
                results["3-stage"][key].append(value)
            results["3-stage"]["latency"].append(latency_3)

            # RRF Hybrid (NEW)
            if include_enhanced:
                results_rrf, latency_rrf = await self.search_rrf_hybrid(query.query_text, query.user_id)
                retrieved_rrf = [r["item_id"] for r in results_rrf]
                metrics_rrf = self._calculate_metrics(retrieved_rrf, relevant_ids)
                for key, value in metrics_rrf.items():
                    results["rrf-hybrid"][key].append(value)
                results["rrf-hybrid"]["latency"].append(latency_rrf)

                # 4-stage (Query Expansion + RRF) (NEW)
                results_4, latency_4 = await self.search_4stage(query.query_text, query.user_id)
                retrieved_4 = [r["item_id"] for r in results_4]
                metrics_4 = self._calculate_metrics(retrieved_4, relevant_ids)
                for key, value in metrics_4.items():
                    results["4-stage"][key].append(value)
                results["4-stage"]["latency"].append(latency_4)

        print("\n")

        # í‰ê·  ê³„ì‚°
        summary = {}
        methods = ["2-stage", "3-stage"]
        if include_enhanced:
            methods.extend(["rrf-hybrid", "4-stage"])

        for method in methods:
            if results[method]["hits"]:  # ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°ë§Œ
                summary[method] = {
                    "hit_rate@1": np.mean(results[method]["hits"]),
                    "mrr": np.mean(results[method]["mrr"]),
                    "ndcg@5": np.mean(results[method]["ndcg"]),
                    "precision@5": np.mean(results[method]["precision"]),
                    "recall@5": np.mean(results[method]["recall"]),
                    "avg_latency_ms": np.mean(results[method]["latency"]),
                    "p95_latency_ms": np.percentile(results[method]["latency"], 95)
                }

        return summary

    def _calculate_metrics(self, retrieved: List[str], relevant: set) -> Dict[str, float]:
        """ë©”íŠ¸ë¦­ ê³„ì‚°"""
        # Hit@1
        hit_at_1 = 1.0 if retrieved and retrieved[0] in relevant else 0.0

        # MRR
        mrr = 0.0
        for i, doc_id in enumerate(retrieved):
            if doc_id in relevant:
                mrr = 1.0 / (i + 1)
                break

        # NDCG@5
        dcg = sum(1.0 / np.log2(i + 2) for i, doc_id in enumerate(retrieved[:5]) if doc_id in relevant)
        idcg = sum(1.0 / np.log2(i + 2) for i in range(min(len(relevant), 5)))
        ndcg = dcg / idcg if idcg > 0 else 0.0

        # Precision@5, Recall@5
        relevant_in_top5 = sum(1 for doc_id in retrieved[:5] if doc_id in relevant)
        precision = relevant_in_top5 / 5
        recall = relevant_in_top5 / len(relevant) if relevant else 0.0

        return {"hits": hit_at_1, "mrr": mrr, "ndcg": ndcg, "precision": precision, "recall": recall}


async def main():
    print("=" * 70)
    print("LaMP (Language Model Personalization) Benchmark v2")
    print("Provider íŒ¨í„´ ê¸°ë°˜ ê°œì¸í™” ì‹œìŠ¤í…œ í‰ê°€")
    print("=" * 70)
    print()

    benchmark = LaMP_Benchmark_V2()

    # ì´ˆê¸°í™”
    await benchmark.initialize()

    # ë°ì´í„° ìƒì„±
    print("ğŸ“Š LaMP ë°ì´í„° ìƒì„± ì¤‘...")
    profiles, queries = benchmark.generate_lamp_data(num_users=10, items_per_user=20)
    print(f"   - ì‚¬ìš©ì: {len(profiles)}")
    print(f"   - í”„ë¡œí•„ ì•„ì´í…œ: {sum(len(p.profile_items) for p in profiles)}")
    print(f"   - ì¿¼ë¦¬: {len(queries)}")
    print()

    # ì¸ë±ì‹±
    await benchmark.setup_collection()
    await benchmark.index_profiles(profiles)
    print()

    # í‰ê°€
    print("ğŸ§ª ë²¤ì¹˜ë§ˆí¬ í‰ê°€ ì‹¤í–‰...")
    print("-" * 70)
    results = await benchmark.evaluate(queries)

    # ê²°ê³¼ ì¶œë ¥
    print("=" * 70)
    print("ğŸ“ˆ LaMP Benchmark ê²°ê³¼")
    print("=" * 70)
    print()

    # 4ê°œ íŒŒì´í”„ë¼ì¸ ë¹„êµ í…Œì´ë¸”
    methods = list(results.keys())
    has_enhanced = "4-stage" in results

    if has_enhanced:
        print("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
        print("â”‚ Metric              â”‚ 2-stage      â”‚ 3-stage      â”‚ rrf-hybrid   â”‚ 4-stage      â”‚")
        print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    else:
        print("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
        print("â”‚ Metric              â”‚ 2-stage          â”‚ 3-stage          â”‚")
        print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")

    metrics = [
        ("Hit Rate@1", "hit_rate@1", "{:.1%}"),
        ("MRR", "mrr", "{:.3f}"),
        ("NDCG@5", "ndcg@5", "{:.3f}"),
        ("Precision@5", "precision@5", "{:.3f}"),
        ("Recall@5", "recall@5", "{:.3f}"),
        ("Avg Latency (ms)", "avg_latency_ms", "{:.1f}"),
        ("P95 Latency (ms)", "p95_latency_ms", "{:.1f}")
    ]

    for label, key, fmt in metrics:
        values = [results[m][key] for m in methods]

        if key.endswith("latency_ms"):
            best_idx = values.index(min(values))
        else:
            best_idx = values.index(max(values))

        row = f"â”‚ {label:<19} â”‚"
        for i, val in enumerate(values):
            val_str = fmt.format(val)
            if i == best_idx:
                val_str = val_str + " âœ“"
            if has_enhanced:
                row += f" {val_str:>12} â”‚"
            else:
                row += f" {val_str:>16} â”‚"
        print(row)

    if has_enhanced:
        print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    else:
        print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    print()

    # 3-stage ëŒ€ë¹„ ê°œì„ ìœ¨ ê³„ì‚°
    baseline = results["3-stage"]
    print("ğŸ“Š 3-stage ëŒ€ë¹„ ê°œì„ ìœ¨:")
    for method in methods:
        if method == "3-stage":
            continue
        r = results[method]
        ndcg_diff = ((r["ndcg@5"] - baseline["ndcg@5"]) / baseline["ndcg@5"]) * 100
        latency_diff = ((baseline["avg_latency_ms"] - r["avg_latency_ms"]) / baseline["avg_latency_ms"]) * 100
        print(f"   {method}: NDCG {ndcg_diff:+.1f}%, Latency {latency_diff:+.1f}%")
    print()

    print("ğŸ“‹ íŒŒì´í”„ë¼ì¸ êµ¬ì„±:")
    print("   2-stage:    Vector Search â†’ Reranking")
    print("   3-stage:    Vector Search â†’ BM25 Hybrid â†’ Reranking")
    print("   rrf-hybrid: Vector + BM25 â†’ RRF Fusion â†’ Reranking (NEW)")
    print("   4-stage:    Query Expansion â†’ Vector â†’ RRF Fusion â†’ Reranking (NEW)")
    print()

    print("ğŸ”§ Provider ì„¤ì •:")
    print(f"   - Embedding: {settings.providers.embedding_provider} ({settings.providers.embedding_model})")
    print(f"   - VectorDB: {settings.providers.vectordb_provider}")
    print(f"   - Reranker: {settings.providers.reranker_provider}")
    print()

    # ê²°ê³¼ ì €ì¥
    output_file = "/tmp/lamp_benchmark_v2_results.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump({
            "benchmark": "LaMP-v2",
            "timestamp": datetime.now().isoformat(),
            "providers": {
                "embedding": settings.providers.embedding_provider,
                "vectordb": settings.providers.vectordb_provider,
                "reranker": settings.providers.reranker_provider
            },
            "config": {
                "num_users": len(profiles),
                "items_per_user": 20,
                "num_queries": len(queries)
            },
            "results": results
        }, f, indent=2, ensure_ascii=False)

    print(f"ğŸ“ ê²°ê³¼ ì €ì¥: {output_file}")


if __name__ == "__main__":
    asyncio.run(main())
