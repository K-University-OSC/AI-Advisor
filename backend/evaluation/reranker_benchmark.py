"""
Reranker Benchmark í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
3ê°œ ë¦¬ë­ì»¤ ë¹„êµ:
1. BAAI/bge-reranker-v2-m3 (ë¡œì»¬, 560MB, ë‹¤êµ­ì–´)
2. BAAI/bge-reranker-v2.5-gemma2-lightweight (ë¡œì»¬, 2.5GB, ê³ ì„±ëŠ¥)
3. Cohere rerank-v3.5 (API, ìœ ë£Œ)

í‰ê°€ ì§€í‘œ: Hit Rate@1, MRR, NDCG@5, Precision@5, Recall@5, Latency
"""

import asyncio
import json
import os
import sys
import time
import math
import random
from datetime import datetime
from typing import List, Dict, Any, Tuple, Optional
from dataclasses import dataclass
from collections import defaultdict
import uuid
import numpy as np

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from dotenv import load_dotenv
load_dotenv()

from openai import OpenAI
from qdrant_client import QdrantClient
from qdrant_client.models import (
    VectorParams, Distance, PointStruct,
    Filter, FieldCondition, MatchValue
)

# Cohere for reranking
try:
    import cohere
    COHERE_AVAILABLE = True
except ImportError:
    COHERE_AVAILABLE = False
    print("âš ï¸ Cohere not installed.")

# BGE Reranker
try:
    from FlagEmbedding import FlagReranker
    BGE_AVAILABLE = True
except ImportError:
    BGE_AVAILABLE = False
    print("âš ï¸ FlagEmbedding not installed.")


@dataclass
class TestQuery:
    """í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬"""
    query_id: str
    user_id: str
    query_text: str
    relevant_doc_ids: List[str]


class RerankerBenchmark:
    """ë¦¬ë­ì»¤ ë²¤ì¹˜ë§ˆí¬ í‰ê°€"""

    def __init__(self):
        self.openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.qdrant = QdrantClient(host="localhost", port=6333)
        self.collection_name = "reranker_benchmark"
        self.embedding_model = "text-embedding-3-large"
        self.embedding_dim = 3072

        # Cohere í´ë¼ì´ì–¸íŠ¸
        self.cohere_client = None
        if COHERE_AVAILABLE and os.getenv("COHERE_API_KEY"):
            self.cohere_client = cohere.Client(os.getenv("COHERE_API_KEY"))
            print("âœ… Cohere client initialized")

        # BGE Rerankers (lazy loading)
        self.bge_m3 = None
        self.bge_gemma = None

        self.documents = []
        self.document_ids = []

    def load_bge_m3(self):
        """BGE Reranker v2-m3 ë¡œë“œ"""
        if self.bge_m3 is None and BGE_AVAILABLE:
            print("ğŸ”„ Loading BGE-Reranker-v2-m3...")
            start = time.time()
            self.bge_m3 = FlagReranker(
                "BAAI/bge-reranker-v2-m3",
                use_fp16=True
            )
            print(f"   âœ… Loaded in {time.time() - start:.1f}s")
        return self.bge_m3

    def load_bge_gemma(self):
        """BGE Reranker v2.5-gemma2-lightweight ë¡œë“œ"""
        if self.bge_gemma is None and BGE_AVAILABLE:
            print("ğŸ”„ Loading BGE-Reranker-v2.5-gemma2-lightweight...")
            start = time.time()
            try:
                # gemma2 ëª¨ë¸ì€ LLM ê¸°ë°˜ì´ë¯€ë¡œ LayerWiseFlagLLMReranker ì‚¬ìš©
                from FlagEmbedding import LayerWiseFlagLLMReranker
                self.bge_gemma = LayerWiseFlagLLMReranker(
                    "BAAI/bge-reranker-v2.5-gemma2-lightweight",
                    use_fp16=True,
                    trust_remote_code=True
                )
                print(f"   âœ… Loaded in {time.time() - start:.1f}s")
            except Exception as e:
                print(f"   âš ï¸ Failed to load gemma2: {e}")
                self.bge_gemma = None
        return self.bge_gemma

    def generate_test_data(self, num_users: int = 10) -> Tuple[List[Dict], List[TestQuery]]:
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±"""
        documents = []
        queries = []

        # ë‹¤ì–‘í•œ ë„ë©”ì¸ì˜ í…ŒìŠ¤íŠ¸ ë°ì´í„°
        test_scenarios = [
            {
                "theme": "ê°œë°œì",
                "docs": [
                    "ë‚˜ëŠ” ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œìë¡œ ì¼í•˜ê³  ìˆì–´ìš”",
                    "Pythonê³¼ JavaScriptë¥¼ ì£¼ë¡œ ì‚¬ìš©í•´ìš”",
                    "ì£¼ë§ì—ëŠ” ì˜¤í”ˆì†ŒìŠ¤ í”„ë¡œì íŠ¸ì— ê¸°ì—¬í•´ìš”",
                    "ì»¤í”¼ë¥¼ ë§ˆì‹œë©´ì„œ ì½”ë”©í•˜ëŠ” ê²ƒì„ ì¢‹ì•„í•´ìš”",
                    "ìµœê·¼ì— AIì™€ ë¨¸ì‹ ëŸ¬ë‹ì— ê´€ì‹¬ì´ ë§ì•„ìš”"
                ],
                "queries": [
                    ("ì–´ë–¤ ì¼ì„ í•˜ì„¸ìš”?", [0]),
                    ("í”„ë¡œê·¸ë˜ë° ì–¸ì–´ ë­ ì¨ìš”?", [1]),
                    ("ì·¨ë¯¸ê°€ ë­ì˜ˆìš”?", [2, 3]),
                    ("ìš”ì¦˜ ë­ì— ê´€ì‹¬ìˆì–´ìš”?", [4]),
                    ("ê°œë°œí•  ë•Œ ë­ ë§ˆì…”ìš”?", [3])
                ]
            },
            {
                "theme": "í•™ìƒ",
                "docs": [
                    "ì €ëŠ” ëŒ€í•™ì—ì„œ ì»´í“¨í„°ê³µí•™ì„ ì „ê³µí•˜ê³  ìˆì–´ìš”",
                    "ì˜¬í•´ ì¡¸ì—… ì˜ˆì •ì´ì—ìš”",
                    "ë™ì•„ë¦¬ì—ì„œ ì•± ê°œë°œì„ í•˜ê³  ìˆì–´ìš”",
                    "ì¥í•™ê¸ˆì„ ë°›ìœ¼ë©° ê³µë¶€í•˜ê³  ìˆì–´ìš”",
                    "ì¡¸ì—… í›„ì—ëŠ” ëŒ€ê¸°ì—…ì— ì·¨ì—…í•˜ê³  ì‹¶ì–´ìš”"
                ],
                "queries": [
                    ("ì „ê³µì´ ë­ì˜ˆìš”?", [0]),
                    ("ëª‡ í•™ë…„ì´ì—ìš”?", [1]),
                    ("ë™ì•„ë¦¬ í™œë™ í•˜ì„¸ìš”?", [2]),
                    ("í•™ë¹„ëŠ” ì–´ë–»ê²Œ í•´ê²°í•´ìš”?", [3]),
                    ("ì¡¸ì—… í›„ ê³„íšì´ ìˆì–´ìš”?", [4])
                ]
            },
            {
                "theme": "ì—¬í–‰ê°€",
                "docs": [
                    "ì—¬í–‰ì„ ì •ë§ ì¢‹ì•„í•´ìš”",
                    "ì‘ë…„ì— ìœ ëŸ½ 5ê°œêµ­ì„ ë‹¤ë…€ì™”ì–´ìš”",
                    "ë‹¤ìŒ ëª©í‘œëŠ” ë‚¨ë¯¸ ì—¬í–‰ì´ì—ìš”",
                    "ì‚¬ì§„ ì°ëŠ” ê²ƒì„ ì¢‹ì•„í•´ì„œ ì—¬í–‰ ì¤‘ ë§ì´ ì°ì–´ìš”",
                    "í˜„ì§€ ìŒì‹ ë¨¹ëŠ” ê²ƒì´ ì—¬í–‰ì˜ ë¬˜ë¯¸ë¼ê³  ìƒê°í•´ìš”"
                ],
                "queries": [
                    ("ì·¨ë¯¸ê°€ ë­ì˜ˆìš”?", [0]),
                    ("ìµœê·¼ì— ì–´ë”” ë‹¤ë…€ì™”ì–´ìš”?", [1]),
                    ("ë‹¤ìŒì— ì–´ë”” ê°€ê³  ì‹¶ì–´ìš”?", [2]),
                    ("ì—¬í–‰ ì¤‘ì— ë­ í•´ìš”?", [3, 4]),
                    ("ì—¬í–‰ì˜ ì¬ë¯¸ê°€ ë­ì˜ˆìš”?", [4])
                ]
            },
            {
                "theme": "ìš”ë¦¬ì‚¬",
                "docs": [
                    "ìš”ë¦¬ì‚¬ë¡œ ì¼í•˜ê³  ìˆì–´ìš”",
                    "ì´íƒˆë¦¬ì•ˆ ìš”ë¦¬ë¥¼ ì „ë¬¸ìœ¼ë¡œ í•´ìš”",
                    "ë‚˜ë§Œì˜ ë ˆìŠ¤í† ë‘ì„ ì—¬ëŠ” ê²ƒì´ ê¿ˆì´ì—ìš”",
                    "ì‹ ì„ í•œ ì¬ë£Œì— ì§‘ì°©í•˜ëŠ” í¸ì´ì—ìš”",
                    "ì£¼ë§ì—ëŠ” ì§‘ì—ì„œ ìƒˆë¡œìš´ ë ˆì‹œí”¼ë¥¼ ì‹¤í—˜í•´ìš”"
                ],
                "queries": [
                    ("ë¬´ìŠ¨ ì¼ í•˜ì„¸ìš”?", [0]),
                    ("ì–´ë–¤ ìš”ë¦¬ë¥¼ ì£¼ë¡œ í•´ìš”?", [1]),
                    ("ê¿ˆì´ ë­ì˜ˆìš”?", [2]),
                    ("ìš”ë¦¬í•  ë•Œ ì¤‘ìš”í•˜ê²Œ ìƒê°í•˜ëŠ” ê²ƒì€?", [3]),
                    ("ì‰¬ëŠ” ë‚ ì—ëŠ” ë­ í•´ìš”?", [4])
                ]
            },
            {
                "theme": "ìŒì•…ê°€",
                "docs": [
                    "ê¸°íƒ€ë¥¼ ì¹˜ëŠ” ê²ƒì„ ì¢‹ì•„í•´ìš”",
                    "ë°´ë“œì—ì„œ ê¸°íƒ€ë¦¬ìŠ¤íŠ¸ë¡œ í™œë™í•˜ê³  ìˆì–´ìš”",
                    "ì£¼ë¡œ ë½ê³¼ ë¸”ë£¨ìŠ¤ë¥¼ ì—°ì£¼í•´ìš”",
                    "ìŒì•…ì€ 10ì‚´ ë•Œë¶€í„° ì‹œì‘í–ˆì–´ìš”",
                    "ì–¸ì  ê°€ ì•¨ë²”ì„ ë‚´ê³  ì‹¶ì–´ìš”"
                ],
                "queries": [
                    ("ì•…ê¸° í•  ì¤„ ì•Œì•„ìš”?", [0]),
                    ("ë°´ë“œ í™œë™ í•˜ì„¸ìš”?", [1]),
                    ("ì–´ë–¤ ì¥ë¥´ë¥¼ ì¢‹ì•„í•´ìš”?", [2]),
                    ("ìŒì•…ì€ ì–¸ì œë¶€í„° í–ˆì–´ìš”?", [3]),
                    ("ìŒì•… ê´€ë ¨ ëª©í‘œê°€ ìˆì–´ìš”?", [4])
                ]
            },
            {
                "theme": "í”¼íŠ¸ë‹ˆìŠ¤",
                "docs": [
                    "í—¬ìŠ¤ì¥ì—ì„œ ì›¨ì´íŠ¸ íŠ¸ë ˆì´ë‹ì„ í•´ìš”",
                    "ë§¤ì¼ ì•„ì¹¨ 5ì‹œì— ì¼ì–´ë‚˜ì„œ ìš´ë™í•´ìš”",
                    "ê±´ê°•í•œ ì‹ë‹¨ ê´€ë¦¬ë„ í•¨ê»˜ í•˜ê³  ìˆì–´ìš”",
                    "ë§ˆë¼í†¤ ëŒ€íšŒì— ì°¸ê°€í•˜ëŠ” ê²ƒì´ ëª©í‘œì˜ˆìš”",
                    "ìš´ë™ í›„ ë‹¨ë°±ì§ˆ ì‰ì´í¬ë¥¼ ê¼­ ë§ˆì…”ìš”"
                ],
                "queries": [
                    ("ìš´ë™ í•˜ì„¸ìš”?", [0]),
                    ("ì–¸ì œ ìš´ë™í•´ìš”?", [1]),
                    ("ì‹ë‹¨ë„ ê´€ë¦¬í•´ìš”?", [2]),
                    ("ìš´ë™ ëª©í‘œê°€ ìˆì–´ìš”?", [3]),
                    ("ìš´ë™ í›„ì— ë­ ë¨¹ì–´ìš”?", [4])
                ]
            },
            {
                "theme": "ê²Œì´ë¨¸",
                "docs": [
                    "ê²Œì„ì„ ì •ë§ ì¢‹ì•„í•´ìš”",
                    "FPSì™€ RPG ì¥ë¥´ë¥¼ ì£¼ë¡œ í•´ìš”",
                    "ì£¼ë§ì—ëŠ” ì¹œêµ¬ë“¤ê³¼ ì˜¨ë¼ì¸ìœ¼ë¡œ ê²Œì„í•´ìš”",
                    "eìŠ¤í¬ì¸  ê²½ê¸° ë³´ëŠ” ê²ƒë„ ì¢‹ì•„í•´ìš”",
                    "ê²Œì„ìš© PCë¥¼ ì§ì ‘ ì¡°ë¦½í–ˆì–´ìš”"
                ],
                "queries": [
                    ("ì·¨ë¯¸ê°€ ë­ì˜ˆìš”?", [0]),
                    ("ì–´ë–¤ ê²Œì„ ì¢‹ì•„í•´ìš”?", [1]),
                    ("ì£¼ë§ì— ë­ í•´ìš”?", [2]),
                    ("eìŠ¤í¬ì¸  ê´€ì‹¬ ìˆì–´ìš”?", [3]),
                    ("PC ì‚¬ì–‘ì´ ì–´ë–»ê²Œ ë¼ìš”?", [4])
                ]
            },
            {
                "theme": "ì˜ˆìˆ ê°€",
                "docs": [
                    "ê·¸ë¦¼ ê·¸ë¦¬ëŠ” ê²ƒì„ ì¢‹ì•„í•´ìš”",
                    "ì£¼ë¡œ ìˆ˜ì±„í™”ì™€ ìœ í™”ë¥¼ ê·¸ë ¤ìš”",
                    "ì „ì‹œíšŒì— ì‘í’ˆì„ ì¶œí’ˆí•œ ì ì´ ìˆì–´ìš”",
                    "ìì—° í’ê²½ì„ ê·¸ë¦¬ëŠ” ê²ƒì„ ì¢‹ì•„í•´ìš”",
                    "ë¯¸ìˆ ê´€ ê°€ëŠ” ê²ƒì„ ì¦ê²¨ìš”"
                ],
                "queries": [
                    ("ì·¨ë¯¸ê°€ ìˆì–´ìš”?", [0]),
                    ("ì–´ë–¤ ê·¸ë¦¼ ê·¸ë ¤ìš”?", [1]),
                    ("ì „ì‹œíšŒ í•´ë³¸ ì  ìˆì–´ìš”?", [2]),
                    ("ì£¼ë¡œ ë­˜ ê·¸ë ¤ìš”?", [3]),
                    ("ì£¼ë§ì— ë­ í•´ìš”?", [4])
                ]
            },
            {
                "theme": "ë…ì„œê°€",
                "docs": [
                    "ë…ì„œë¥¼ ì •ë§ ì¢‹ì•„í•´ìš”",
                    "í•œ ë‹¬ì— ì±…ì„ 4-5ê¶Œ ì½ì–´ìš”",
                    "ì¶”ë¦¬ì†Œì„¤ê³¼ SFë¥¼ ì£¼ë¡œ ì½ì–´ìš”",
                    "ë„ì„œê´€ì— ìì£¼ ê°€ìš”",
                    "ë…ì„œ ëª¨ì„ì— ì°¸ì—¬í•˜ê³  ìˆì–´ìš”"
                ],
                "queries": [
                    ("ì·¨ë¯¸ê°€ ë­ì˜ˆìš”?", [0]),
                    ("ì±… ë§ì´ ì½ì–´ìš”?", [1]),
                    ("ì–´ë–¤ ì¥ë¥´ ì¢‹ì•„í•´ìš”?", [2]),
                    ("ì±…ì€ ì–´ë””ì„œ ë¹Œë ¤ìš”?", [3]),
                    ("ë…ì„œ ëª¨ì„ ê°™ì€ ê±° í•´ìš”?", [4])
                ]
            },
            {
                "theme": "ë°˜ë ¤ë™ë¬¼",
                "docs": [
                    "ê³ ì–‘ì´ ë‘ ë§ˆë¦¬ë¥¼ í‚¤ìš°ê³  ìˆì–´ìš”",
                    "ê³ ì–‘ì´ ì´ë¦„ì€ ë‚˜ë¹„ì™€ ì½©ì´ì—ìš”",
                    "ë§¤ì¼ ì•„ì¹¨ì €ë…ìœ¼ë¡œ ë°¥ì„ ì±™ê²¨ì¤˜ìš”",
                    "ì£¼ë§ì—ëŠ” í•¨ê»˜ ë†€ì•„ì¤˜ìš”",
                    "ê³ ì–‘ì´ ìš©í’ˆì— ëˆì„ ë§ì´ ì¨ìš”"
                ],
                "queries": [
                    ("ë°˜ë ¤ë™ë¬¼ ìˆì–´ìš”?", [0]),
                    ("ì´ë¦„ì´ ë­ì˜ˆìš”?", [1]),
                    ("ëŒë³´ëŠ” ê²Œ í˜ë“¤ì§€ ì•Šì•„ìš”?", [2]),
                    ("ì£¼ë§ì— ë­ í•´ìš”?", [3]),
                    ("ë¹„ìš©ì´ ë§ì´ ë“¤ì–´ìš”?", [4])
                ]
            }
        ]

        for user_idx in range(min(num_users, len(test_scenarios))):
            scenario = test_scenarios[user_idx]
            user_id = f"user_{user_idx + 1}"

            # ë¬¸ì„œ ìƒì„±
            for doc_idx, doc_text in enumerate(scenario["docs"]):
                doc_id = f"{user_id}_doc_{doc_idx}"
                documents.append({
                    "doc_id": doc_id,
                    "user_id": user_id,
                    "text": doc_text
                })

            # ì¿¼ë¦¬ ìƒì„±
            for q_idx, (query_text, relevant_indices) in enumerate(scenario["queries"]):
                queries.append(TestQuery(
                    query_id=f"{user_id}_query_{q_idx}",
                    user_id=user_id,
                    query_text=query_text,
                    relevant_doc_ids=[f"{user_id}_doc_{i}" for i in relevant_indices]
                ))

        return documents, queries

    def get_embedding(self, text: str) -> List[float]:
        """OpenAI ì„ë² ë”© ìƒì„±"""
        response = self.openai_client.embeddings.create(
            model=self.embedding_model,
            input=text
        )
        return response.data[0].embedding

    def setup_collection(self):
        """Qdrant ì»¬ë ‰ì…˜ ì„¤ì •"""
        collections = self.qdrant.get_collections().collections
        if any(c.name == self.collection_name for c in collections):
            self.qdrant.delete_collection(self.collection_name)

        self.qdrant.create_collection(
            collection_name=self.collection_name,
            vectors_config=VectorParams(
                size=self.embedding_dim,
                distance=Distance.COSINE
            )
        )
        print(f"âœ… Collection '{self.collection_name}' ìƒì„± ì™„ë£Œ")

    def index_documents(self, documents: List[Dict]):
        """ë¬¸ì„œ ì¸ë±ì‹±"""
        points = []
        self.documents = []
        self.document_ids = []

        for doc in documents:
            embedding = self.get_embedding(doc["text"])

            point = PointStruct(
                id=str(uuid.uuid4()),
                vector=embedding,
                payload={
                    "doc_id": doc["doc_id"],
                    "user_id": doc["user_id"],
                    "text": doc["text"]
                }
            )
            points.append(point)
            self.documents.append(doc["text"])
            self.document_ids.append(doc["doc_id"])

        # Qdrantì— ì—…ë¡œë“œ
        batch_size = 100
        for i in range(0, len(points), batch_size):
            self.qdrant.upsert(
                collection_name=self.collection_name,
                points=points[i:i+batch_size]
            )

        print(f"âœ… {len(points)}ê°œ ë¬¸ì„œ ì¸ë±ì‹± ì™„ë£Œ")

    def vector_search(self, query: str, user_id: str, top_k: int = 15) -> List[Dict]:
        """Vector Search (ê³µí†µ)"""
        query_embedding = self.get_embedding(query)

        search_result = self.qdrant.query_points(
            collection_name=self.collection_name,
            query=query_embedding,
            query_filter=Filter(
                must=[FieldCondition(key="user_id", match=MatchValue(value=user_id))]
            ),
            limit=top_k
        )

        return [
            {
                "doc_id": r.payload["doc_id"],
                "text": r.payload["text"],
                "score": r.score
            }
            for r in search_result.points
        ]

    def rerank_with_cohere(self, query: str, candidates: List[Dict], top_k: int = 5) -> Tuple[List[Dict], float]:
        """Cohere Reranking"""
        if not self.cohere_client or not candidates:
            return candidates[:top_k], 0.0

        start_time = time.time()
        try:
            # Rate limiting
            time.sleep(6.5)

            rerank_response = self.cohere_client.rerank(
                model="rerank-v3.5",
                query=query,
                documents=[c["text"] for c in candidates],
                top_n=top_k
            )

            reranked = []
            for r in rerank_response.results:
                candidate = candidates[r.index].copy()
                candidate["rerank_score"] = r.relevance_score
                reranked.append(candidate)

            latency = (time.time() - start_time) * 1000
            return reranked, latency
        except Exception as e:
            print(f"   Cohere error: {e}")
            return candidates[:top_k], 0.0

    def rerank_with_bge_m3(self, query: str, candidates: List[Dict], top_k: int = 5) -> Tuple[List[Dict], float]:
        """BGE Reranker v2-m3 Reranking"""
        reranker = self.load_bge_m3()
        if not reranker or not candidates:
            return candidates[:top_k], 0.0

        start_time = time.time()
        try:
            pairs = [[query, c["text"]] for c in candidates]
            scores = reranker.compute_score(pairs, normalize=True)

            if isinstance(scores, (int, float)):
                scores = [scores]

            scored = list(zip(range(len(candidates)), scores))
            scored.sort(key=lambda x: x[1], reverse=True)

            reranked = []
            for idx, score in scored[:top_k]:
                candidate = candidates[idx].copy()
                candidate["rerank_score"] = float(score)
                reranked.append(candidate)

            latency = (time.time() - start_time) * 1000
            return reranked, latency
        except Exception as e:
            print(f"   BGE-m3 error: {e}")
            return candidates[:top_k], 0.0

    def rerank_with_bge_gemma(self, query: str, candidates: List[Dict], top_k: int = 5) -> Tuple[List[Dict], float]:
        """BGE Reranker v2.5-gemma2-lightweight Reranking"""
        reranker = self.load_bge_gemma()
        if not reranker or not candidates:
            return candidates[:top_k], 0.0

        start_time = time.time()
        try:
            pairs = [[query, c["text"]] for c in candidates]
            scores = reranker.compute_score(pairs, normalize=True)

            if isinstance(scores, (int, float)):
                scores = [scores]

            scored = list(zip(range(len(candidates)), scores))
            scored.sort(key=lambda x: x[1], reverse=True)

            reranked = []
            for idx, score in scored[:top_k]:
                candidate = candidates[idx].copy()
                candidate["rerank_score"] = float(score)
                reranked.append(candidate)

            latency = (time.time() - start_time) * 1000
            return reranked, latency
        except Exception as e:
            print(f"   BGE-gemma error: {e}")
            return candidates[:top_k], 0.0

    def evaluate_reranker(
        self,
        reranker_name: str,
        rerank_func,
        queries: List[TestQuery],
        top_k: int = 5
    ) -> Dict[str, float]:
        """ë¦¬ë­ì»¤ í‰ê°€"""
        results = {
            "hits": [],
            "mrr": [],
            "ndcg": [],
            "precision": [],
            "recall": [],
            "latency": []
        }

        total = len(queries)
        print(f"\nğŸ“Š {reranker_name} í‰ê°€ ì¤‘...")

        for idx, query in enumerate(queries):
            print(f"\r   ì§„í–‰: {idx+1}/{total}", end="", flush=True)

            # Vector Search
            candidates = self.vector_search(query.query_text, query.user_id, top_k=15)

            # Reranking
            reranked, latency = rerank_func(query.query_text, candidates, top_k)

            # ë©”íŠ¸ë¦­ ê³„ì‚°
            retrieved_ids = [r["doc_id"] for r in reranked]
            relevant_ids = set(query.relevant_doc_ids)

            metrics = self._calculate_metrics(retrieved_ids, relevant_ids)
            results["hits"].append(metrics["hit@1"])
            results["mrr"].append(metrics["mrr"])
            results["ndcg"].append(metrics["ndcg@5"])
            results["precision"].append(metrics["precision@5"])
            results["recall"].append(metrics["recall@5"])
            results["latency"].append(latency)

        print()

        # í‰ê·  ê³„ì‚°
        return {
            "hit_rate@1": np.mean(results["hits"]),
            "mrr": np.mean(results["mrr"]),
            "ndcg@5": np.mean(results["ndcg"]),
            "precision@5": np.mean(results["precision"]),
            "recall@5": np.mean(results["recall"]),
            "avg_latency_ms": np.mean(results["latency"]),
            "p95_latency_ms": np.percentile(results["latency"], 95) if results["latency"] else 0
        }

    def _calculate_metrics(self, retrieved: List[str], relevant: set) -> Dict[str, float]:
        """ë©”íŠ¸ë¦­ ê³„ì‚°"""
        # Hit@1
        hit_at_1 = 1.0 if retrieved and retrieved[0] in relevant else 0.0

        # MRR
        mrr = 0.0
        for i, doc_id in enumerate(retrieved):
            if doc_id in relevant:
                mrr = 1.0 / (i + 1)
                break

        # NDCG@5
        dcg = 0.0
        for i, doc_id in enumerate(retrieved[:5]):
            if doc_id in relevant:
                dcg += 1.0 / np.log2(i + 2)

        idcg = sum(1.0 / np.log2(i + 2) for i in range(min(len(relevant), 5)))
        ndcg = dcg / idcg if idcg > 0 else 0.0

        # Precision@5
        relevant_in_top5 = sum(1 for doc_id in retrieved[:5] if doc_id in relevant)
        precision = relevant_in_top5 / 5

        # Recall@5
        recall = relevant_in_top5 / len(relevant) if relevant else 0.0

        return {
            "hit@1": hit_at_1,
            "mrr": mrr,
            "ndcg@5": ndcg,
            "precision@5": precision,
            "recall@5": recall
        }

    def run_benchmark(self) -> Dict[str, Any]:
        """ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        print("=" * 70)
        print("Reranker Benchmark")
        print("BGE-v2-m3 vs BGE-v2.5-gemma2 vs Cohere rerank-v3.5")
        print("=" * 70)
        print()

        # 1. ë°ì´í„° ìƒì„±
        print("ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì¤‘...")
        documents, queries = self.generate_test_data(num_users=10)
        print(f"   - ë¬¸ì„œ ìˆ˜: {len(documents)}")
        print(f"   - ì¿¼ë¦¬ ìˆ˜: {len(queries)}")
        print()

        # 2. ì¸ë±ì‹±
        print("ğŸ”§ Qdrant ì»¬ë ‰ì…˜ ì„¤ì • ë° ì¸ë±ì‹±...")
        self.setup_collection()
        self.index_documents(documents)
        print()

        # 3. ê° ë¦¬ë­ì»¤ í‰ê°€
        results = {}

        # Vector Only (baseline)
        print("ğŸ“Š Vector Only (baseline) í‰ê°€ ì¤‘...")
        vector_results = {"hits": [], "mrr": [], "ndcg": [], "precision": [], "recall": [], "latency": []}
        for idx, query in enumerate(queries):
            print(f"\r   ì§„í–‰: {idx+1}/{len(queries)}", end="", flush=True)
            start = time.time()
            candidates = self.vector_search(query.query_text, query.user_id, top_k=5)
            latency = (time.time() - start) * 1000

            retrieved_ids = [r["doc_id"] for r in candidates]
            relevant_ids = set(query.relevant_doc_ids)
            metrics = self._calculate_metrics(retrieved_ids, relevant_ids)

            vector_results["hits"].append(metrics["hit@1"])
            vector_results["mrr"].append(metrics["mrr"])
            vector_results["ndcg"].append(metrics["ndcg@5"])
            vector_results["precision"].append(metrics["precision@5"])
            vector_results["recall"].append(metrics["recall@5"])
            vector_results["latency"].append(latency)
        print()

        results["Vector Only"] = {
            "hit_rate@1": np.mean(vector_results["hits"]),
            "mrr": np.mean(vector_results["mrr"]),
            "ndcg@5": np.mean(vector_results["ndcg"]),
            "precision@5": np.mean(vector_results["precision"]),
            "recall@5": np.mean(vector_results["recall"]),
            "avg_latency_ms": np.mean(vector_results["latency"]),
            "p95_latency_ms": np.percentile(vector_results["latency"], 95)
        }

        # BGE Reranker v2-m3
        if BGE_AVAILABLE:
            results["BGE-v2-m3"] = self.evaluate_reranker(
                "BGE-Reranker-v2-m3",
                self.rerank_with_bge_m3,
                queries
            )

        # BGE Reranker v2.5-gemma2-lightweight (transformers ë²„ì „ í˜¸í™˜ì„± ë¬¸ì œë¡œ ë¹„í™œì„±í™”)
        # í˜„ì¬ transformers ë²„ì „ì—ì„œ Gemma2FlashAttention2 import ì˜¤ë¥˜ ë°œìƒ
        # í–¥í›„ FlagEmbedding ë˜ëŠ” transformers ì—…ë°ì´íŠ¸ í›„ í™œì„±í™” í•„ìš”
        # if BGE_AVAILABLE:
        #     results["BGE-v2.5-gemma2"] = self.evaluate_reranker(
        #         "BGE-Reranker-v2.5-gemma2-lightweight",
        #         self.rerank_with_bge_gemma,
        #         queries
        #     )

        # Cohere rerank-v3.5
        if self.cohere_client:
            results["Cohere"] = self.evaluate_reranker(
                "Cohere rerank-v3.5",
                self.rerank_with_cohere,
                queries
            )

        return {
            "results": results,
            "config": {
                "num_documents": len(documents),
                "num_queries": len(queries),
                "embedding_model": self.embedding_model
            }
        }


def print_results(benchmark_results: Dict[str, Any]):
    """ê²°ê³¼ ì¶œë ¥"""
    results = benchmark_results["results"]

    print("\n" + "=" * 90)
    print("ğŸ“ˆ Reranker Benchmark ê²°ê³¼")
    print("=" * 90)
    print()

    # í—¤ë”
    rerankers = list(results.keys())
    header = "â”‚ Metric              â”‚"
    for r in rerankers:
        header += f" {r:^18} â”‚"
    separator = "â”œ" + "â”€" * 21 + "â”¼" + "â”¼".join(["â”€" * 20] * len(rerankers)) + "â”¤"

    print("â”Œ" + "â”€" * 21 + "â”¬" + "â”¬".join(["â”€" * 20] * len(rerankers)) + "â”")
    print(header)
    print(separator)

    # ë©”íŠ¸ë¦­
    metrics_config = [
        ("Hit Rate@1", "hit_rate@1", "{:.1%}"),
        ("MRR", "mrr", "{:.3f}"),
        ("NDCG@5", "ndcg@5", "{:.3f}"),
        ("Precision@5", "precision@5", "{:.3f}"),
        ("Recall@5", "recall@5", "{:.3f}"),
        ("Avg Latency (ms)", "avg_latency_ms", "{:.1f}"),
        ("P95 Latency (ms)", "p95_latency_ms", "{:.1f}")
    ]

    for label, key, fmt in metrics_config:
        row = f"â”‚ {label:<19} â”‚"
        values = [results[r].get(key, 0) for r in rerankers]
        best_idx = values.index(max(values)) if key != "avg_latency_ms" and key != "p95_latency_ms" else values.index(min(values))

        for idx, (r, val) in enumerate(zip(rerankers, values)):
            formatted = fmt.format(val)
            if idx == best_idx and len(rerankers) > 1:
                row += f" {formatted:>14} ğŸ†  â”‚"
            else:
                row += f" {formatted:>18} â”‚"
        print(row)

    print("â””" + "â”€" * 21 + "â”´" + "â”´".join(["â”€" * 20] * len(rerankers)) + "â”˜")
    print()

    # ë¶„ì„
    print("ğŸ“‹ ë¦¬ë­ì»¤ ë¹„êµ:")
    print("   â€¢ Vector Only: ë²¡í„° ê²€ìƒ‰ë§Œ (baseline)")
    print("   â€¢ BGE-v2-m3: BAAI/bge-reranker-v2-m3 (560MB, ë‹¤êµ­ì–´, ë¬´ë£Œ)")
    print("   â€¢ BGE-v2.5-gemma2: BAAI/bge-reranker-v2.5-gemma2-lightweight (2.5GB, ê³ ì„±ëŠ¥, ë¬´ë£Œ)")
    print("   â€¢ Cohere: rerank-v3.5 (API, $2/1000 searches)")
    print()

    # ìµœê³  ì„±ëŠ¥ ë¶„ì„
    if len(results) > 1:
        print("ğŸ“Š ë¶„ì„:")
        best_hit = max(results.items(), key=lambda x: x[1].get("hit_rate@1", 0))
        best_mrr = max(results.items(), key=lambda x: x[1].get("mrr", 0))
        fastest = min(results.items(), key=lambda x: x[1].get("avg_latency_ms", float("inf")))

        print(f"   â€¢ ìµœê³  Hit Rate@1: {best_hit[0]} ({best_hit[1]['hit_rate@1']:.1%})")
        print(f"   â€¢ ìµœê³  MRR: {best_mrr[0]} ({best_mrr[1]['mrr']:.3f})")
        print(f"   â€¢ ìµœì € Latency: {fastest[0]} ({fastest[1]['avg_latency_ms']:.1f}ms)")
        print()


async def main():
    benchmark = RerankerBenchmark()
    results = benchmark.run_benchmark()

    print_results(results)

    # ê²°ê³¼ ì €ì¥
    output_file = "/tmp/reranker_benchmark_results.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump({
            "benchmark": "Reranker Comparison",
            "timestamp": datetime.now().isoformat(),
            **results
        }, f, indent=2, ensure_ascii=False)

    print(f"ğŸ“ ê²°ê³¼ ì €ì¥: {output_file}")


if __name__ == "__main__":
    asyncio.run(main())
