"""
PersonaChat Benchmark í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
2-stage vs 3-stage íŒŒì´í”„ë¼ì¸ ë¹„êµ

PersonaChat (Facebook AI Research):
- í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ ëŒ€í™” ë°ì´í„°ì…‹
- ê° ëŒ€í™”ìê°€ 4-5ê°œì˜ í˜ë¥´ì†Œë‚˜ ë¬¸ì¥ì„ ê°€ì§
- í˜ë¥´ì†Œë‚˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¼ê´€ëœ ëŒ€í™”ë¥¼ ìƒì„±
- ê²€ìƒ‰ ì¦ê°• ìƒì„±(RAG)ì—ì„œ ì˜¬ë°”ë¥¸ í˜ë¥´ì†Œë‚˜ ê²€ìƒ‰ì´ í•µì‹¬

í‰ê°€ íƒœìŠ¤í¬: ì£¼ì–´ì§„ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ì—ì„œ ê´€ë ¨ í˜ë¥´ì†Œë‚˜ ë¬¸ì¥ ê²€ìƒ‰
"""

import asyncio
import json
import os
import sys
import time
import math
import random
from datetime import datetime, timedelta
from typing import List, Dict, Any, Tuple, Optional
from dataclasses import dataclass
from collections import defaultdict
import uuid
import numpy as np

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from dotenv import load_dotenv
load_dotenv()

from openai import OpenAI
from qdrant_client import QdrantClient
from qdrant_client.models import (
    VectorParams, Distance, PointStruct,
    Filter, FieldCondition, MatchValue
)

# Cohere for reranking
try:
    import cohere
    COHERE_AVAILABLE = True
except ImportError:
    COHERE_AVAILABLE = False
    print("âš ï¸ Cohere not installed. Reranking will be disabled.")


@dataclass
class Persona:
    """í˜ë¥´ì†Œë‚˜ ë°ì´í„°"""
    persona_id: str
    user_id: str
    statements: List[str]  # í˜ë¥´ì†Œë‚˜ ë¬¸ì¥ë“¤ (4-5ê°œ)


@dataclass
class PersonaChatQuery:
    """PersonaChat ì¿¼ë¦¬"""
    query_id: str
    user_id: str
    dialogue_context: str  # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸
    query_utterance: str   # í˜„ì¬ ë°œí™”
    relevant_persona_ids: List[str]  # ê´€ë ¨ í˜ë¥´ì†Œë‚˜ ë¬¸ì¥ ì¸ë±ìŠ¤


class BM25:
    """BM25 ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜"""

    def __init__(self, k1: float = 1.5, b: float = 0.75):
        self.k1 = k1
        self.b = b
        self.doc_freqs = {}
        self.doc_lens = []
        self.avg_doc_len = 0
        self.corpus_size = 0
        self.documents = []
        self.tokenized_docs = []

    def tokenize(self, text: str) -> List[str]:
        import re
        text = text.lower()
        tokens = re.findall(r'[ê°€-í£]+|[a-z0-9]+', text)
        return tokens

    def fit(self, documents: List[str]):
        self.documents = documents
        self.corpus_size = len(documents)
        self.tokenized_docs = [self.tokenize(doc) for doc in documents]
        self.doc_lens = [len(doc) for doc in self.tokenized_docs]
        self.avg_doc_len = sum(self.doc_lens) / self.corpus_size if self.corpus_size > 0 else 0

        self.doc_freqs = defaultdict(int)
        for doc in self.tokenized_docs:
            for term in set(doc):
                self.doc_freqs[term] += 1

    def get_scores(self, query: str) -> List[float]:
        query_tokens = self.tokenize(query)
        scores = []

        for idx, doc in enumerate(self.tokenized_docs):
            score = 0.0
            doc_len = self.doc_lens[idx]
            term_freqs = defaultdict(int)
            for term in doc:
                term_freqs[term] += 1

            for term in query_tokens:
                if term not in term_freqs:
                    continue
                tf = term_freqs[term]
                df = self.doc_freqs.get(term, 0)
                if df == 0:
                    continue
                idf = math.log((self.corpus_size - df + 0.5) / (df + 0.5) + 1)
                tf_norm = (tf * (self.k1 + 1)) / (tf + self.k1 * (1 - self.b + self.b * doc_len / self.avg_doc_len))
                score += idf * tf_norm
            scores.append(score)

        return scores

    def search(self, query: str, top_k: int = 10) -> List[Tuple[int, float]]:
        scores = self.get_scores(query)
        indexed_scores = [(i, score) for i, score in enumerate(scores)]
        indexed_scores.sort(key=lambda x: x[1], reverse=True)
        return indexed_scores[:top_k]


class PersonaChatBenchmark:
    """PersonaChat ë²¤ì¹˜ë§ˆí¬ í‰ê°€"""

    def __init__(self):
        self.openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.qdrant = QdrantClient(host="localhost", port=6333)
        self.collection_name = "personachat_benchmark"
        self.embedding_model = "text-embedding-3-large"
        self.embedding_dim = 3072

        self.cohere_client = None
        if COHERE_AVAILABLE and os.getenv("COHERE_API_KEY"):
            self.cohere_client = cohere.Client(os.getenv("COHERE_API_KEY"))
            print("âœ… Cohere Reranking í™œì„±í™”")
        else:
            print("âš ï¸ Cohere Reranking ë¹„í™œì„±í™”")

        self.bm25 = BM25()
        self.documents = []
        self.document_ids = []

    def generate_personachat_data(self, num_users: int = 15) -> Tuple[List[Persona], List[PersonaChatQuery]]:
        """
        PersonaChat ìŠ¤íƒ€ì¼ì˜ í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ ëŒ€í™” ë°ì´í„° ìƒì„±
        """
        personas = []
        queries = []

        # ë‹¤ì–‘í•œ í˜ë¥´ì†Œë‚˜ í…œí”Œë¦¿
        persona_templates = [
            {
                "theme": "developer",
                "statements": [
                    "ë‚˜ëŠ” ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œìë¡œ ì¼í•˜ê³  ìˆì–´ìš”",
                    "Pythonê³¼ JavaScriptë¥¼ ì£¼ë¡œ ì‚¬ìš©í•´ìš”",
                    "ì£¼ë§ì—ëŠ” ì˜¤í”ˆì†ŒìŠ¤ í”„ë¡œì íŠ¸ì— ê¸°ì—¬í•´ìš”",
                    "ì»¤í”¼ë¥¼ ë§ˆì‹œë©´ì„œ ì½”ë”©í•˜ëŠ” ê²ƒì„ ì¢‹ì•„í•´ìš”",
                    "ìµœê·¼ì— AIì™€ ë¨¸ì‹ ëŸ¬ë‹ì— ê´€ì‹¬ì´ ë§ì•„ìš”"
                ],
                "dialogues": [
                    ("ì–´ë–¤ ì¼ì„ í•˜ì„¸ìš”?", [0]),
                    ("í”„ë¡œê·¸ë˜ë° ì–¸ì–´ ë­ ì¨ìš”?", [1]),
                    ("ì·¨ë¯¸ê°€ ë­ì˜ˆìš”?", [2, 3]),
                    ("ìš”ì¦˜ ë­ì— ê´€ì‹¬ìˆì–´ìš”?", [4]),
                    ("ê°œë°œí•  ë•Œ ë­ ë§ˆì…”ìš”?", [3])
                ]
            },
            {
                "theme": "student",
                "statements": [
                    "ì €ëŠ” ëŒ€í•™ì—ì„œ ì»´í“¨í„°ê³µí•™ì„ ì „ê³µí•˜ê³  ìˆì–´ìš”",
                    "ì˜¬í•´ ì¡¸ì—… ì˜ˆì •ì´ì—ìš”",
                    "ë™ì•„ë¦¬ì—ì„œ ì•± ê°œë°œì„ í•˜ê³  ìˆì–´ìš”",
                    "ì¥í•™ê¸ˆì„ ë°›ìœ¼ë©° ê³µë¶€í•˜ê³  ìˆì–´ìš”",
                    "ì¡¸ì—… í›„ì—ëŠ” ëŒ€ê¸°ì—…ì— ì·¨ì—…í•˜ê³  ì‹¶ì–´ìš”"
                ],
                "dialogues": [
                    ("ì „ê³µì´ ë­ì˜ˆìš”?", [0]),
                    ("ëª‡ í•™ë…„ì´ì—ìš”?", [1]),
                    ("ë™ì•„ë¦¬ í™œë™ í•˜ì„¸ìš”?", [2]),
                    ("í•™ë¹„ëŠ” ì–´ë–»ê²Œ í•´ê²°í•´ìš”?", [3]),
                    ("ì¡¸ì—… í›„ ê³„íšì´ ìˆì–´ìš”?", [4])
                ]
            },
            {
                "theme": "traveler",
                "statements": [
                    "ì—¬í–‰ì„ ì •ë§ ì¢‹ì•„í•´ìš”",
                    "ì‘ë…„ì— ìœ ëŸ½ 5ê°œêµ­ì„ ë‹¤ë…€ì™”ì–´ìš”",
                    "ë‹¤ìŒ ëª©í‘œëŠ” ë‚¨ë¯¸ ì—¬í–‰ì´ì—ìš”",
                    "ì‚¬ì§„ ì°ëŠ” ê²ƒì„ ì¢‹ì•„í•´ì„œ ì—¬í–‰ ì¤‘ ë§ì´ ì°ì–´ìš”",
                    "í˜„ì§€ ìŒì‹ ë¨¹ëŠ” ê²ƒì´ ì—¬í–‰ì˜ ë¬˜ë¯¸ë¼ê³  ìƒê°í•´ìš”"
                ],
                "dialogues": [
                    ("ì·¨ë¯¸ê°€ ë­ì˜ˆìš”?", [0]),
                    ("ìµœê·¼ì— ì–´ë”” ë‹¤ë…€ì™”ì–´ìš”?", [1]),
                    ("ë‹¤ìŒì— ì–´ë”” ê°€ê³  ì‹¶ì–´ìš”?", [2]),
                    ("ì—¬í–‰ ì¤‘ì— ë­ í•´ìš”?", [3, 4]),
                    ("ì—¬í–‰ì˜ ì¬ë¯¸ê°€ ë­ì˜ˆìš”?", [4])
                ]
            },
            {
                "theme": "chef",
                "statements": [
                    "ìš”ë¦¬ì‚¬ë¡œ ì¼í•˜ê³  ìˆì–´ìš”",
                    "ì´íƒˆë¦¬ì•ˆ ìš”ë¦¬ë¥¼ ì „ë¬¸ìœ¼ë¡œ í•´ìš”",
                    "ë‚˜ë§Œì˜ ë ˆìŠ¤í† ë‘ì„ ì—¬ëŠ” ê²ƒì´ ê¿ˆì´ì—ìš”",
                    "ì‹ ì„ í•œ ì¬ë£Œì— ì§‘ì°©í•˜ëŠ” í¸ì´ì—ìš”",
                    "ì£¼ë§ì—ëŠ” ì§‘ì—ì„œ ìƒˆë¡œìš´ ë ˆì‹œí”¼ë¥¼ ì‹¤í—˜í•´ìš”"
                ],
                "dialogues": [
                    ("ë¬´ìŠ¨ ì¼ í•˜ì„¸ìš”?", [0]),
                    ("ì–´ë–¤ ìš”ë¦¬ë¥¼ ì£¼ë¡œ í•´ìš”?", [1]),
                    ("ê¿ˆì´ ë­ì˜ˆìš”?", [2]),
                    ("ìš”ë¦¬í•  ë•Œ ì¤‘ìš”í•˜ê²Œ ìƒê°í•˜ëŠ” ê²ƒì€?", [3]),
                    ("ì‰¬ëŠ” ë‚ ì—ëŠ” ë­ í•´ìš”?", [4])
                ]
            },
            {
                "theme": "musician",
                "statements": [
                    "ê¸°íƒ€ë¥¼ ì¹˜ëŠ” ê²ƒì„ ì¢‹ì•„í•´ìš”",
                    "ë°´ë“œì—ì„œ ê¸°íƒ€ë¦¬ìŠ¤íŠ¸ë¡œ í™œë™í•˜ê³  ìˆì–´ìš”",
                    "ì£¼ë¡œ ë½ê³¼ ë¸”ë£¨ìŠ¤ë¥¼ ì—°ì£¼í•´ìš”",
                    "ìŒì•…ì€ 10ì‚´ ë•Œë¶€í„° ì‹œì‘í–ˆì–´ìš”",
                    "ì–¸ì  ê°€ ì•¨ë²”ì„ ë‚´ê³  ì‹¶ì–´ìš”"
                ],
                "dialogues": [
                    ("ì•…ê¸° í•  ì¤„ ì•Œì•„ìš”?", [0]),
                    ("ë°´ë“œ í™œë™ í•˜ì„¸ìš”?", [1]),
                    ("ì–´ë–¤ ì¥ë¥´ë¥¼ ì¢‹ì•„í•´ìš”?", [2]),
                    ("ìŒì•…ì€ ì–¸ì œë¶€í„° í–ˆì–´ìš”?", [3]),
                    ("ìŒì•… ê´€ë ¨ ëª©í‘œê°€ ìˆì–´ìš”?", [4])
                ]
            },
            {
                "theme": "fitness",
                "statements": [
                    "í—¬ìŠ¤ì¥ì—ì„œ ì›¨ì´íŠ¸ íŠ¸ë ˆì´ë‹ì„ í•´ìš”",
                    "ë§¤ì¼ ì•„ì¹¨ 5ì‹œì— ì¼ì–´ë‚˜ì„œ ìš´ë™í•´ìš”",
                    "ê±´ê°•í•œ ì‹ë‹¨ ê´€ë¦¬ë„ í•¨ê»˜ í•˜ê³  ìˆì–´ìš”",
                    "ë§ˆë¼í†¤ ëŒ€íšŒì— ì°¸ê°€í•˜ëŠ” ê²ƒì´ ëª©í‘œì˜ˆìš”",
                    "ìš´ë™ í›„ ë‹¨ë°±ì§ˆ ì‰ì´í¬ë¥¼ ê¼­ ë§ˆì…”ìš”"
                ],
                "dialogues": [
                    ("ìš´ë™ í•˜ì„¸ìš”?", [0]),
                    ("ì–¸ì œ ìš´ë™í•´ìš”?", [1]),
                    ("ì‹ë‹¨ë„ ê´€ë¦¬í•´ìš”?", [2]),
                    ("ìš´ë™ ëª©í‘œê°€ ìˆì–´ìš”?", [3]),
                    ("ìš´ë™ í›„ì— ë­ ë¨¹ì–´ìš”?", [4])
                ]
            },
            {
                "theme": "gamer",
                "statements": [
                    "ê²Œì„ì„ ì •ë§ ì¢‹ì•„í•´ìš”",
                    "FPSì™€ RPG ì¥ë¥´ë¥¼ ì£¼ë¡œ í•´ìš”",
                    "ì£¼ë§ì—ëŠ” ì¹œêµ¬ë“¤ê³¼ ì˜¨ë¼ì¸ìœ¼ë¡œ ê²Œì„í•´ìš”",
                    "eìŠ¤í¬ì¸  ê²½ê¸° ë³´ëŠ” ê²ƒë„ ì¢‹ì•„í•´ìš”",
                    "ê²Œì„ìš© PCë¥¼ ì§ì ‘ ì¡°ë¦½í–ˆì–´ìš”"
                ],
                "dialogues": [
                    ("ì·¨ë¯¸ê°€ ë­ì˜ˆìš”?", [0]),
                    ("ì–´ë–¤ ê²Œì„ ì¢‹ì•„í•´ìš”?", [1]),
                    ("ì£¼ë§ì— ë­ í•´ìš”?", [2]),
                    ("eìŠ¤í¬ì¸  ê´€ì‹¬ ìˆì–´ìš”?", [3]),
                    ("PC ì‚¬ì–‘ì´ ì–´ë–»ê²Œ ë¼ìš”?", [4])
                ]
            },
            {
                "theme": "artist",
                "statements": [
                    "ê·¸ë¦¼ ê·¸ë¦¬ëŠ” ê²ƒì„ ì¢‹ì•„í•´ìš”",
                    "ì£¼ë¡œ ìˆ˜ì±„í™”ì™€ ìœ í™”ë¥¼ ê·¸ë ¤ìš”",
                    "ì „ì‹œíšŒì— ì‘í’ˆì„ ì¶œí’ˆí•œ ì ì´ ìˆì–´ìš”",
                    "ìì—° í’ê²½ì„ ê·¸ë¦¬ëŠ” ê²ƒì„ ì¢‹ì•„í•´ìš”",
                    "ë¯¸ìˆ ê´€ ê°€ëŠ” ê²ƒì„ ì¦ê²¨ìš”"
                ],
                "dialogues": [
                    ("ì·¨ë¯¸ê°€ ìˆì–´ìš”?", [0]),
                    ("ì–´ë–¤ ê·¸ë¦¼ ê·¸ë ¤ìš”?", [1]),
                    ("ì „ì‹œíšŒ í•´ë³¸ ì  ìˆì–´ìš”?", [2]),
                    ("ì£¼ë¡œ ë­˜ ê·¸ë ¤ìš”?", [3]),
                    ("ì£¼ë§ì— ë­ í•´ìš”?", [4])
                ]
            },
            {
                "theme": "reader",
                "statements": [
                    "ë…ì„œë¥¼ ì •ë§ ì¢‹ì•„í•´ìš”",
                    "í•œ ë‹¬ì— ì±…ì„ 4-5ê¶Œ ì½ì–´ìš”",
                    "ì¶”ë¦¬ì†Œì„¤ê³¼ SFë¥¼ ì£¼ë¡œ ì½ì–´ìš”",
                    "ë„ì„œê´€ì— ìì£¼ ê°€ìš”",
                    "ë…ì„œ ëª¨ì„ì— ì°¸ì—¬í•˜ê³  ìˆì–´ìš”"
                ],
                "dialogues": [
                    ("ì·¨ë¯¸ê°€ ë­ì˜ˆìš”?", [0]),
                    ("ì±… ë§ì´ ì½ì–´ìš”?", [1]),
                    ("ì–´ë–¤ ì¥ë¥´ ì¢‹ì•„í•´ìš”?", [2]),
                    ("ì±…ì€ ì–´ë””ì„œ ë¹Œë ¤ìš”?", [3]),
                    ("ë…ì„œ ëª¨ì„ ê°™ì€ ê±° í•´ìš”?", [4])
                ]
            },
            {
                "theme": "pet_owner",
                "statements": [
                    "ê³ ì–‘ì´ ë‘ ë§ˆë¦¬ë¥¼ í‚¤ìš°ê³  ìˆì–´ìš”",
                    "ê³ ì–‘ì´ ì´ë¦„ì€ ë‚˜ë¹„ì™€ ì½©ì´ì—ìš”",
                    "ë§¤ì¼ ì•„ì¹¨ì €ë…ìœ¼ë¡œ ë°¥ì„ ì±™ê²¨ì¤˜ìš”",
                    "ì£¼ë§ì—ëŠ” í•¨ê»˜ ë†€ì•„ì¤˜ìš”",
                    "ê³ ì–‘ì´ ìš©í’ˆì— ëˆì„ ë§ì´ ì¨ìš”"
                ],
                "dialogues": [
                    ("ë°˜ë ¤ë™ë¬¼ ìˆì–´ìš”?", [0]),
                    ("ì´ë¦„ì´ ë­ì˜ˆìš”?", [1]),
                    ("ëŒë³´ëŠ” ê²Œ í˜ë“¤ì§€ ì•Šì•„ìš”?", [2]),
                    ("ì£¼ë§ì— ë­ í•´ìš”?", [3]),
                    ("ë¹„ìš©ì´ ë§ì´ ë“¤ì–´ìš”?", [4])
                ]
            },
            {
                "theme": "coffee_lover",
                "statements": [
                    "ì»¤í”¼ë¥¼ ì •ë§ ì¢‹ì•„í•´ìš”",
                    "í•˜ë£¨ì— 3ì”ì€ ê¼­ ë§ˆì…”ìš”",
                    "ì§‘ì—ì„œ ì§ì ‘ ì›ë‘ë¥¼ ê°ˆì•„ì„œ ë‚´ë ¤ìš”",
                    "ì¹´í˜ íˆ¬ì–´ë¥¼ ì¦ê²¨ìš”",
                    "ë¼ë–¼ì•„íŠ¸ë¥¼ ë°°ìš°ê³  ìˆì–´ìš”"
                ],
                "dialogues": [
                    ("ì»¤í”¼ ì¢‹ì•„í•´ìš”?", [0]),
                    ("í•˜ë£¨ì— ëª‡ ì” ë§ˆì…”ìš”?", [1]),
                    ("ì§‘ì—ì„œ ì»¤í”¼ ë‚´ë ¤ìš”?", [2]),
                    ("ì¹´í˜ ìì£¼ ê°€ìš”?", [3]),
                    ("ë°”ë¦¬ìŠ¤íƒ€ì— ê´€ì‹¬ ìˆì–´ìš”?", [4])
                ]
            },
            {
                "theme": "movie_buff",
                "statements": [
                    "ì˜í™”ë¥¼ ì •ë§ ì¢‹ì•„í•´ìš”",
                    "ì¼ì£¼ì¼ì— 2-3í¸ì€ ë´ìš”",
                    "ìŠ¤ë¦´ëŸ¬ì™€ SF ì¥ë¥´ë¥¼ ì¢‹ì•„í•´ìš”",
                    "ì˜í™”ê´€ì—ì„œ ë³´ëŠ” ê²ƒì„ ì„ í˜¸í•´ìš”",
                    "ì˜í™” ë¦¬ë·° ë¸”ë¡œê·¸ë¥¼ ìš´ì˜í•´ìš”"
                ],
                "dialogues": [
                    ("ì·¨ë¯¸ê°€ ë­ì˜ˆìš”?", [0]),
                    ("ì˜í™” ë§ì´ ë´ìš”?", [1]),
                    ("ì–´ë–¤ ì¥ë¥´ ì¢‹ì•„í•´ìš”?", [2]),
                    ("OTTë¡œ ë´ìš”, ì˜í™”ê´€ ê°€ìš”?", [3]),
                    ("ì˜í™” ê´€ë ¨ í™œë™ í•´ìš”?", [4])
                ]
            },
            {
                "theme": "entrepreneur",
                "statements": [
                    "ìŠ¤íƒ€íŠ¸ì—…ì„ ìš´ì˜í•˜ê³  ìˆì–´ìš”",
                    "IT ì„œë¹„ìŠ¤ ë¶„ì•¼ì—ì„œ ì¼í•´ìš”",
                    "íŒ€ì›ì´ 10ëª… ì •ë„ ìˆì–´ìš”",
                    "íˆ¬ìë¥¼ ë°›ì•„ì„œ ì„±ì¥ ì¤‘ì´ì—ìš”",
                    "ì›Œë¼ë°¸ë³´ë‹¤ëŠ” ì¼ì— ì§‘ì¤‘í•˜ê³  ìˆì–´ìš”"
                ],
                "dialogues": [
                    ("ë¬´ìŠ¨ ì¼ í•˜ì„¸ìš”?", [0]),
                    ("ì–´ë–¤ ë¶„ì•¼ì˜ˆìš”?", [1]),
                    ("íšŒì‚¬ ê·œëª¨ê°€ ì–´ë–»ê²Œ ë¼ìš”?", [2]),
                    ("ì‚¬ì—…ì€ ì˜ ë˜ê³  ìˆì–´ìš”?", [3]),
                    ("ì¼í•˜ëŠë¼ ë°”ì˜ì‹œê² ì–´ìš”", [4])
                ]
            },
            {
                "theme": "language_learner",
                "statements": [
                    "ì™¸êµ­ì–´ ë°°ìš°ëŠ” ê²ƒì„ ì¢‹ì•„í•´ìš”",
                    "ì§€ê¸ˆ ì¼ë³¸ì–´ë¥¼ ê³µë¶€í•˜ê³  ìˆì–´ìš”",
                    "ì˜ì–´, ì¤‘êµ­ì–´ëŠ” ì´ë¯¸ í•  ì¤„ ì•Œì•„ìš”",
                    "ì–¸ì–´ êµí™˜ ì•±ì„ ì‚¬ìš©í•´ìš”",
                    "ëª©í‘œëŠ” 5ê°œ êµ­ì–´ë¥¼ í•˜ëŠ” ê±°ì˜ˆìš”"
                ],
                "dialogues": [
                    ("ì·¨ë¯¸ê°€ ë­ì˜ˆìš”?", [0]),
                    ("ì§€ê¸ˆ ë­ ë°°ìš°ê³  ìˆì–´ìš”?", [1]),
                    ("ì™¸êµ­ì–´ ëª‡ ê°œ í•´ìš”?", [2]),
                    ("ì–´ë–»ê²Œ ê³µë¶€í•´ìš”?", [3]),
                    ("ì–¸ì–´ ê´€ë ¨ ëª©í‘œê°€ ìˆì–´ìš”?", [4])
                ]
            },
            {
                "theme": "gardener",
                "statements": [
                    "ì •ì› ê°€ê¾¸ëŠ” ê²ƒì„ ì¢‹ì•„í•´ìš”",
                    "ë² ë€ë‹¤ì—ì„œ ì±„ì†Œë¥¼ í‚¤ì›Œìš”",
                    "í† ë§ˆí† ì™€ í—ˆë¸Œë¥¼ ì£¼ë¡œ í‚¤ì›Œìš”",
                    "ë§¤ì¼ ì•„ì¹¨ ë¬¼ì„ ì¤˜ìš”",
                    "ì§ì ‘ í‚¤ìš´ ì±„ì†Œë¡œ ìš”ë¦¬í•˜ëŠ” ê²Œ ë³´ëŒìˆì–´ìš”"
                ],
                "dialogues": [
                    ("ì·¨ë¯¸ê°€ ìˆì–´ìš”?", [0]),
                    ("ì§‘ì—ì„œ ë­”ê°€ í‚¤ì›Œìš”?", [1]),
                    ("ì–´ë–¤ ì‹ë¬¼ í‚¤ì›Œìš”?", [2]),
                    ("ê´€ë¦¬í•˜ê¸° í˜ë“¤ì§€ ì•Šì•„ìš”?", [3]),
                    ("ìˆ˜í™•í•˜ë©´ ì–´ë–»ê²Œ í•´ìš”?", [4])
                ]
            }
        ]

        for user_idx in range(min(num_users, len(persona_templates))):
            template = persona_templates[user_idx]
            user_id = f"user_{user_idx + 1}"

            # í˜ë¥´ì†Œë‚˜ ìƒì„±
            persona_statements = []
            for stmt_idx, stmt in enumerate(template["statements"]):
                persona_id = f"{user_id}_persona_{stmt_idx}"
                persona_statements.append({
                    "id": persona_id,
                    "text": stmt
                })

            personas.append(Persona(
                persona_id=f"{user_id}_persona",
                user_id=user_id,
                statements=[s["text"] for s in persona_statements]
            ))

            # ì¿¼ë¦¬ ìƒì„±
            for q_idx, (dialogue, relevant_indices) in enumerate(template["dialogues"]):
                query = PersonaChatQuery(
                    query_id=f"{user_id}_query_{q_idx}",
                    user_id=user_id,
                    dialogue_context=f"ëŒ€í™” ìƒëŒ€: {dialogue}",
                    query_utterance=dialogue,
                    relevant_persona_ids=[f"{user_id}_persona_{i}" for i in relevant_indices]
                )
                queries.append(query)

        return personas, queries

    def get_embedding(self, text: str) -> List[float]:
        response = self.openai_client.embeddings.create(
            model=self.embedding_model,
            input=text
        )
        return response.data[0].embedding

    def setup_collection(self):
        collections = self.qdrant.get_collections().collections
        if any(c.name == self.collection_name for c in collections):
            self.qdrant.delete_collection(self.collection_name)

        self.qdrant.create_collection(
            collection_name=self.collection_name,
            vectors_config=VectorParams(
                size=self.embedding_dim,
                distance=Distance.COSINE
            )
        )
        print(f"âœ… Collection '{self.collection_name}' ìƒì„± ì™„ë£Œ")

    def index_personas(self, personas: List[Persona]):
        points = []
        self.documents = []
        self.document_ids = []

        for persona in personas:
            for stmt_idx, statement in enumerate(persona.statements):
                persona_id = f"{persona.user_id}_persona_{stmt_idx}"
                embedding = self.get_embedding(statement)

                point = PointStruct(
                    id=str(uuid.uuid4()),
                    vector=embedding,
                    payload={
                        "persona_id": persona_id,
                        "user_id": persona.user_id,
                        "text": statement,
                        "statement_index": stmt_idx
                    }
                )
                points.append(point)
                self.documents.append(statement)
                self.document_ids.append(persona_id)

        # Qdrantì— ì—…ë¡œë“œ
        batch_size = 100
        for i in range(0, len(points), batch_size):
            self.qdrant.upsert(
                collection_name=self.collection_name,
                points=points[i:i+batch_size]
            )

        # BM25 ì¸ë±ìŠ¤ êµ¬ì¶•
        self.bm25.fit(self.documents)

        print(f"âœ… {len(points)}ê°œ í˜ë¥´ì†Œë‚˜ ë¬¸ì¥ ì¸ë±ì‹± ì™„ë£Œ")

    def search_before(self, query: str, user_id: str, top_k: int = 5) -> Tuple[List[Dict], float]:
        """Before (2-stage): Vector Search â†’ Cohere Reranking"""
        start_time = time.time()

        query_embedding = self.get_embedding(query)

        search_result = self.qdrant.query_points(
            collection_name=self.collection_name,
            query=query_embedding,
            query_filter=Filter(
                must=[FieldCondition(key="user_id", match=MatchValue(value=user_id))]
            ),
            limit=top_k * 3
        )
        results = search_result.points

        candidates = [
            {
                "persona_id": r.payload["persona_id"],
                "text": r.payload["text"],
                "score": r.score
            }
            for r in results
        ]

        # Cohere Reranking
        if self.cohere_client and candidates:
            try:
                time.sleep(3.5)  # Rate limiting

                rerank_response = self.cohere_client.rerank(
                    model="rerank-v3.5",
                    query=query,
                    documents=[c["text"] for c in candidates],
                    top_n=top_k
                )

                reranked = []
                for r in rerank_response.results:
                    candidate = candidates[r.index]
                    candidate["rerank_score"] = r.relevance_score
                    reranked.append(candidate)
                candidates = reranked
            except Exception as e:
                if "429" not in str(e):
                    print(f"Reranking error: {e}")
                candidates = candidates[:top_k]
        else:
            candidates = candidates[:top_k]

        latency = (time.time() - start_time) * 1000
        return candidates, latency

    def search_after(self, query: str, user_id: str, top_k: int = 5) -> Tuple[List[Dict], float]:
        """After (3-stage): Vector Search â†’ BM25 Hybrid â†’ Cohere Reranking"""
        start_time = time.time()

        query_embedding = self.get_embedding(query)

        search_result = self.qdrant.query_points(
            collection_name=self.collection_name,
            query=query_embedding,
            query_filter=Filter(
                must=[FieldCondition(key="user_id", match=MatchValue(value=user_id))]
            ),
            limit=top_k * 3
        )
        vector_results = search_result.points

        vector_candidates = {
            r.payload["persona_id"]: {
                "persona_id": r.payload["persona_id"],
                "text": r.payload["text"],
                "vector_score": r.score
            }
            for r in vector_results
        }

        # BM25 ê²€ìƒ‰
        bm25_results = self.bm25.search(query, top_k=top_k * 3)

        # í•´ë‹¹ ì‚¬ìš©ìì˜ ë¬¸ì„œë§Œ í•„í„°ë§
        user_doc_indices = [
            i for i, doc_id in enumerate(self.document_ids)
            if doc_id.startswith(user_id)
        ]

        bm25_filtered = [
            (idx, score) for idx, score in bm25_results
            if idx in user_doc_indices
        ]

        # BM25 ì ìˆ˜ ì •ê·œí™” ë° í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜
        if bm25_filtered:
            max_bm25 = max(score for _, score in bm25_filtered) if bm25_filtered else 1
            for idx, bm25_score in bm25_filtered:
                persona_id = self.document_ids[idx]
                if persona_id in vector_candidates:
                    vector_candidates[persona_id]["bm25_score"] = bm25_score / max_bm25 if max_bm25 > 0 else 0

        for persona_id, candidate in vector_candidates.items():
            vector_score = candidate.get("vector_score", 0)
            bm25_score = candidate.get("bm25_score", 0)
            candidate["hybrid_score"] = 0.7 * vector_score + 0.3 * bm25_score

        candidates = sorted(
            vector_candidates.values(),
            key=lambda x: x.get("hybrid_score", 0),
            reverse=True
        )[:top_k * 2]

        # Cohere Reranking
        if self.cohere_client and candidates:
            try:
                time.sleep(3.5)  # Rate limiting

                rerank_response = self.cohere_client.rerank(
                    model="rerank-v3.5",
                    query=query,
                    documents=[c["text"] for c in candidates],
                    top_n=top_k
                )

                reranked = []
                for r in rerank_response.results:
                    candidate = candidates[r.index]
                    candidate["rerank_score"] = r.relevance_score
                    reranked.append(candidate)
                candidates = reranked
            except Exception as e:
                if "429" not in str(e):
                    print(f"Reranking error: {e}")
                candidates = candidates[:top_k]
        else:
            candidates = candidates[:top_k]

        latency = (time.time() - start_time) * 1000
        return candidates, latency

    def evaluate(self, queries: List[PersonaChatQuery], personas: List[Persona]) -> Dict[str, Any]:
        """ë²¤ì¹˜ë§ˆí¬ í‰ê°€"""
        results = {
            "before": {"hits": [], "mrr": [], "ndcg": [], "precision": [], "recall": [], "latency": []},
            "after": {"hits": [], "mrr": [], "ndcg": [], "precision": [], "recall": [], "latency": []}
        }

        total = len(queries)

        for idx, query in enumerate(queries):
            print(f"\rí‰ê°€ ì¤‘... {idx+1}/{total}", end="", flush=True)

            relevant_ids = set(query.relevant_persona_ids)

            # Before (2-stage)
            before_results, before_latency = self.search_before(query.query_utterance, query.user_id)
            before_retrieved = [r["persona_id"] for r in before_results]

            before_metrics = self._calculate_metrics(before_retrieved, relevant_ids)
            results["before"]["hits"].append(before_metrics["hit@1"])
            results["before"]["mrr"].append(before_metrics["mrr"])
            results["before"]["ndcg"].append(before_metrics["ndcg@5"])
            results["before"]["precision"].append(before_metrics["precision@5"])
            results["before"]["recall"].append(before_metrics["recall@5"])
            results["before"]["latency"].append(before_latency)

            # After (3-stage)
            after_results, after_latency = self.search_after(query.query_utterance, query.user_id)
            after_retrieved = [r["persona_id"] for r in after_results]

            after_metrics = self._calculate_metrics(after_retrieved, relevant_ids)
            results["after"]["hits"].append(after_metrics["hit@1"])
            results["after"]["mrr"].append(after_metrics["mrr"])
            results["after"]["ndcg"].append(after_metrics["ndcg@5"])
            results["after"]["precision"].append(after_metrics["precision@5"])
            results["after"]["recall"].append(after_metrics["recall@5"])
            results["after"]["latency"].append(after_latency)

        print("\n")

        # í‰ê·  ê³„ì‚°
        summary = {}
        for method in ["before", "after"]:
            summary[method] = {
                "hit_rate@1": np.mean(results[method]["hits"]),
                "mrr": np.mean(results[method]["mrr"]),
                "ndcg@5": np.mean(results[method]["ndcg"]),
                "precision@5": np.mean(results[method]["precision"]),
                "recall@5": np.mean(results[method]["recall"]),
                "avg_latency_ms": np.mean(results[method]["latency"]),
                "p95_latency_ms": np.percentile(results[method]["latency"], 95)
            }

        return summary

    def _calculate_metrics(self, retrieved: List[str], relevant: set) -> Dict[str, float]:
        # Hit@1
        hit_at_1 = 1.0 if retrieved and retrieved[0] in relevant else 0.0

        # MRR
        mrr = 0.0
        for i, doc_id in enumerate(retrieved):
            if doc_id in relevant:
                mrr = 1.0 / (i + 1)
                break

        # NDCG@5
        dcg = 0.0
        for i, doc_id in enumerate(retrieved[:5]):
            if doc_id in relevant:
                dcg += 1.0 / np.log2(i + 2)

        idcg = sum(1.0 / np.log2(i + 2) for i in range(min(len(relevant), 5)))
        ndcg = dcg / idcg if idcg > 0 else 0.0

        # Precision@5
        relevant_in_top5 = sum(1 for doc_id in retrieved[:5] if doc_id in relevant)
        precision = relevant_in_top5 / 5

        # Recall@5
        recall = relevant_in_top5 / len(relevant) if relevant else 0.0

        return {
            "hit@1": hit_at_1,
            "mrr": mrr,
            "ndcg@5": ndcg,
            "precision@5": precision,
            "recall@5": recall
        }


async def main():
    print("=" * 70)
    print("PersonaChat Benchmark")
    print("2-stage vs 3-stage íŒŒì´í”„ë¼ì¸ ë¹„êµ")
    print("=" * 70)
    print()

    benchmark = PersonaChatBenchmark()

    # 1. ë°ì´í„° ìƒì„±
    print("ğŸ“Š PersonaChat ìŠ¤íƒ€ì¼ ë°ì´í„° ìƒì„± ì¤‘...")
    personas, queries = benchmark.generate_personachat_data(num_users=15)
    print(f"   - ì‚¬ìš©ì ìˆ˜: {len(personas)}")
    print(f"   - ì´ í˜ë¥´ì†Œë‚˜ ë¬¸ì¥: {sum(len(p.statements) for p in personas)}")
    print(f"   - ì¿¼ë¦¬ ìˆ˜: {len(queries)}")
    print()

    # 2. ì¸ë±ì‹±
    print("ğŸ”§ Qdrant ì»¬ë ‰ì…˜ ì„¤ì • ë° ì¸ë±ì‹±...")
    benchmark.setup_collection()
    benchmark.index_personas(personas)
    print()

    # 3. í‰ê°€
    print("ğŸ§ª ë²¤ì¹˜ë§ˆí¬ í‰ê°€ ì‹¤í–‰...")
    print("-" * 70)
    results = benchmark.evaluate(queries, personas)

    # 4. ê²°ê³¼ ì¶œë ¥
    print("=" * 70)
    print("ğŸ“ˆ PersonaChat Benchmark ê²°ê³¼")
    print("=" * 70)
    print()

    print("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("â”‚ Metric              â”‚ Before (2-stage) â”‚ After (3-stage)  â”‚")
    print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")

    metrics = [
        ("Hit Rate@1", "hit_rate@1", "{:.1%}"),
        ("MRR", "mrr", "{:.3f}"),
        ("NDCG@5", "ndcg@5", "{:.3f}"),
        ("Precision@5", "precision@5", "{:.3f}"),
        ("Recall@5", "recall@5", "{:.3f}"),
        ("Avg Latency (ms)", "avg_latency_ms", "{:.1f}"),
        ("P95 Latency (ms)", "p95_latency_ms", "{:.1f}")
    ]

    for label, key, fmt in metrics:
        before_val = results["before"][key]
        after_val = results["after"][key]

        if key.endswith("latency_ms"):
            diff = before_val - after_val
            indicator = "â¬‡ï¸" if diff > 0 else "â¬†ï¸"
        else:
            diff = after_val - before_val
            indicator = "â¬†ï¸" if diff > 0 else "â¬‡ï¸" if diff < 0 else "â¡ï¸"

        before_str = fmt.format(before_val)
        after_str = fmt.format(after_val) + f" {indicator}"

        print(f"â”‚ {label:<19} â”‚ {before_str:>16} â”‚ {after_str:>16} â”‚")

    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    print()

    print("ğŸ“‹ íŒŒì´í”„ë¼ì¸ êµ¬ì„±:")
    print("   Before (2-stage): Vector Search â†’ Cohere Reranking")
    print("   After (3-stage):  Vector Search â†’ BM25 Hybrid â†’ Cohere Reranking")
    print()

    # ë¶„ì„
    print("ğŸ“Š ë¶„ì„:")
    hit_diff = (results["after"]["hit_rate@1"] - results["before"]["hit_rate@1"]) * 100
    mrr_diff = (results["after"]["mrr"] - results["before"]["mrr"]) * 100
    latency_diff = results["after"]["avg_latency_ms"] - results["before"]["avg_latency_ms"]

    if hit_diff > 0:
        print(f"   âœ… Hit Rate@1ì´ {hit_diff:.1f}%p í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤.")
    elif hit_diff < 0:
        print(f"   âš ï¸ Hit Rate@1ì´ {abs(hit_diff):.1f}%p ê°ì†Œí–ˆìŠµë‹ˆë‹¤.")
    else:
        print(f"   â¡ï¸ Hit Rate@1ì€ ë™ì¼í•©ë‹ˆë‹¤.")

    if mrr_diff > 0:
        print(f"   âœ… MRRì´ {mrr_diff:.1f}%p í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤.")
    elif mrr_diff < 0:
        print(f"   âš ï¸ MRRì´ {abs(mrr_diff):.1f}%p ê°ì†Œí–ˆìŠµë‹ˆë‹¤.")

    print(f"   â±ï¸ LatencyëŠ” {latency_diff:+.1f}ms ì°¨ì´ë‚©ë‹ˆë‹¤.")
    print()

    print("ğŸ’¡ PersonaChat ë²¤ì¹˜ë§ˆí¬ íŠ¹ì„±:")
    print("   - í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ ëŒ€í™”ì—ì„œ ì˜¬ë°”ë¥¸ í˜ë¥´ì†Œë‚˜ ë¬¸ì¥ ê²€ìƒ‰")
    print("   - ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ì™€ í˜ë¥´ì†Œë‚˜ ê°„ì˜ ì˜ë¯¸ì  ì—°ê²°ì´ ì¤‘ìš”")
    print("   - í‚¤ì›Œë“œ ë§¤ì¹­ë„ í˜ë¥´ì†Œë‚˜ ê²€ìƒ‰ì— ë„ì›€ì´ ë  ìˆ˜ ìˆìŒ")
    print()

    # ê²°ê³¼ ì €ì¥
    output_file = "/tmp/personachat_benchmark_results.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump({
            "benchmark": "PersonaChat",
            "timestamp": datetime.now().isoformat(),
            "config": {
                "num_users": len(personas),
                "num_persona_statements": sum(len(p.statements) for p in personas),
                "num_queries": len(queries),
                "embedding_model": "text-embedding-3-large",
                "reranking": "cohere/rerank-v3.5"
            },
            "results": results
        }, f, indent=2, ensure_ascii=False)

    print(f"ğŸ“ ê²°ê³¼ ì €ì¥: {output_file}")


if __name__ == "__main__":
    asyncio.run(main())
