"""
LLM-as-Judge ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸
ê°œì¸í™” í–¥ìƒ ëª¨ë“ˆ (ì‹œë§¨í‹± ë¼ìš°í„°, Self-RAG, LLM-Judge) ì„±ëŠ¥ í‰ê°€

í‰ê°€ í•­ëª©:
1. ì‹œë§¨í‹± ë¼ìš°í„° ì •í™•ë„
2. Self-RAG ê´€ë ¨ì„± íŒë‹¨ ì •í™•ë„
3. LLM-as-Judge í‰ê°€ ì¼ê´€ì„±
"""

import asyncio
import json
import os
import sys
import time
from datetime import datetime
from typing import List, Dict, Any
from dataclasses import dataclass

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from dotenv import load_dotenv
load_dotenv()

# ê°œì¸í™” í–¥ìƒ ëª¨ë“ˆ import
from services.personalization_enhancer import (
    get_personalization_enhancer,
    QueryIntent,
    SemanticRouter,
    SelfRAG,
    LLMJudge,
    HierarchicalMemoryManager
)


@dataclass
class TestCase:
    """í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤"""
    query: str
    expected_intent: QueryIntent
    context: str = ""
    response: str = ""
    is_relevant: bool = True


class LLMJudgeBenchmark:
    """LLM-as-Judge ë²¤ì¹˜ë§ˆí¬"""

    def __init__(self):
        self.enhancer = None
        self.semantic_router = None
        self.self_rag = None
        self.llm_judge = None
        self.results = {}

    async def initialize(self):
        """ì´ˆê¸°í™”"""
        print("ğŸ”§ ê°œì¸í™” í–¥ìƒ ëª¨ë“ˆ ì´ˆê¸°í™” ì¤‘...")
        self.enhancer = get_personalization_enhancer()
        # ê°œë³„ ì»´í¬ë„ŒíŠ¸ ìƒì„±
        self.semantic_router = SemanticRouter()
        await self.semantic_router.initialize()
        self.self_rag = SelfRAG()
        self.llm_judge = LLMJudge()
        print("   âœ… ì´ˆê¸°í™” ì™„ë£Œ")

    def generate_test_cases(self) -> List[TestCase]:
        """í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„±"""
        test_cases = [
            # GREETING í…ŒìŠ¤íŠ¸
            TestCase("ì•ˆë…•í•˜ì„¸ìš”!", QueryIntent.GREETING),
            TestCase("ì¢‹ì€ ì•„ì¹¨ì´ì—ìš”", QueryIntent.GREETING),
            TestCase("ë°˜ê°‘ìŠµë‹ˆë‹¤", QueryIntent.GREETING),
            TestCase("ì˜¤ëœë§Œì´ì—ìš”", QueryIntent.GREETING),
            TestCase("Hi there!", QueryIntent.GREETING),

            # PERSONAL í…ŒìŠ¤íŠ¸
            TestCase("ë‚´ê°€ ì§€ë‚œë²ˆì— ë¬¼ì–´ë³¸ ê±° ê¸°ì–µë‚˜?", QueryIntent.PERSONAL),
            TestCase("ì €ë²ˆì— ì¶”ì²œí•´ì¤€ ì±… ë­ì˜€ì§€?", QueryIntent.PERSONAL),
            TestCase("ë‚´ í”„ë¡œí•„ ì„¤ì • ë°”ê¿”ì¤˜", QueryIntent.PERSONAL),
            TestCase("ë‚˜í•œí…Œ ë§ëŠ” ì¶”ì²œí•´ì¤˜", QueryIntent.PERSONAL),
            TestCase("ë‚´ í•™ìŠµ ê¸°ë¡ ë³´ì—¬ì¤˜", QueryIntent.PERSONAL),

            # GENERAL í…ŒìŠ¤íŠ¸
            TestCase("Pythonì—ì„œ ë¦¬ìŠ¤íŠ¸ ì •ë ¬í•˜ëŠ” ë°©ë²•ì´ ë­ì•¼?", QueryIntent.GENERAL),
            TestCase("ë¨¸ì‹ ëŸ¬ë‹ì´ ë­ì•¼?", QueryIntent.GENERAL),
            TestCase("Reactì™€ Vueì˜ ì°¨ì´ì ì´ ë­ì•¼?", QueryIntent.GENERAL),
            TestCase("SQL ì¡°ì¸ ì¢…ë¥˜ ì•Œë ¤ì¤˜", QueryIntent.GENERAL),
            TestCase("ê°ì²´ì§€í–¥ í”„ë¡œê·¸ë˜ë°ì´ ë­ì•¼?", QueryIntent.GENERAL),

            # ACTION í…ŒìŠ¤íŠ¸
            TestCase("ì´ ì½”ë“œ ë¶„ì„í•´ì¤˜", QueryIntent.ACTION),
            TestCase("ì´ í•¨ìˆ˜ ìµœì í™”í•´ì¤˜", QueryIntent.ACTION),
            TestCase("ë²„ê·¸ ì°¾ì•„ì¤˜", QueryIntent.ACTION),
            TestCase("í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‘ì„±í•´ì¤˜", QueryIntent.ACTION),
            TestCase("ë¦¬íŒ©í† ë§ í•´ì¤˜", QueryIntent.ACTION),

            # CLARIFICATION í…ŒìŠ¤íŠ¸
            TestCase("ì¢€ ë” ìì„¸íˆ ì„¤ëª…í•´ì¤˜", QueryIntent.CLARIFICATION),
            TestCase("ì˜ˆì‹œë¥¼ ë“¤ì–´ì¤„ë˜?", QueryIntent.CLARIFICATION),
            TestCase("ë‹¤ë¥¸ ë°©ë²•ì€ ì—†ì–´?", QueryIntent.CLARIFICATION),
            TestCase("ì™œ ê·¸ë ‡ê²Œ ë˜ëŠ” ê±°ì•¼?", QueryIntent.CLARIFICATION),
            TestCase("ë¬´ìŠ¨ ë§ì´ì•¼?", QueryIntent.CLARIFICATION),
        ]
        return test_cases

    def generate_relevance_cases(self) -> List[Dict]:
        """ê´€ë ¨ì„± í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„±"""
        cases = [
            # ê´€ë ¨ì„± ë†’ìŒ
            {
                "query": "Pythonì—ì„œ ë¦¬ìŠ¤íŠ¸ ì •ë ¬í•˜ëŠ” ë°©ë²•",
                "context": "Pythonì—ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ëŠ” ë°©ë²•ì€ ì—¬ëŸ¬ ê°€ì§€ê°€ ìˆìŠµë‹ˆë‹¤. sort() ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ë©´ ì›ë³¸ ë¦¬ìŠ¤íŠ¸ê°€ ì •ë ¬ë˜ê³ , sorted() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ ìƒˆë¡œìš´ ì •ë ¬ëœ ë¦¬ìŠ¤íŠ¸ê°€ ë°˜í™˜ë©ë‹ˆë‹¤.",
                "expected_relevant": True
            },
            {
                "query": "ë¨¸ì‹ ëŸ¬ë‹ ê¸°ì´ˆ ê°œë…",
                "context": "ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ í•™ìŠµí•˜ì—¬ ì˜ˆì¸¡ì´ë‚˜ ê²°ì •ì„ ë‚´ë¦¬ëŠ” ì¸ê³µì§€ëŠ¥ì˜ í•œ ë¶„ì•¼ì…ë‹ˆë‹¤. ì§€ë„í•™ìŠµ, ë¹„ì§€ë„í•™ìŠµ, ê°•í™”í•™ìŠµ ë“±ì˜ ë°©ë²•ë¡ ì´ ìˆìŠµë‹ˆë‹¤.",
                "expected_relevant": True
            },
            {
                "query": "React ìƒíƒœ ê´€ë¦¬",
                "context": "Reactì—ì„œ ìƒíƒœ ê´€ë¦¬ëŠ” useState, useReducer í›…ì„ ì‚¬ìš©í•˜ê±°ë‚˜ Redux, MobX ê°™ì€ ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                "expected_relevant": True
            },
            # ê´€ë ¨ì„± ë‚®ìŒ
            {
                "query": "Pythonì—ì„œ ë¦¬ìŠ¤íŠ¸ ì •ë ¬í•˜ëŠ” ë°©ë²•",
                "context": "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ìŠµë‹ˆë‹¤. ì‚°ì±…í•˜ê¸° ì¢‹ì€ ë‚ ì´ë„¤ìš”.",
                "expected_relevant": False
            },
            {
                "query": "ë°ì´í„°ë² ì´ìŠ¤ ì¸ë±ì‹±",
                "context": "ìš”ë¦¬ë¥¼ í•  ë•ŒëŠ” ì¬ë£Œ ì¤€ë¹„ê°€ ì¤‘ìš”í•©ë‹ˆë‹¤. ì‹ ì„ í•œ ì¬ë£Œë¥¼ ì‚¬ìš©í•˜ë©´ ë§›ì´ ì¢‹ìŠµë‹ˆë‹¤.",
                "expected_relevant": False
            },
            {
                "query": "API ì„¤ê³„ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤",
                "context": "ìš´ë™ì„ ê·œì¹™ì ìœ¼ë¡œ í•˜ë©´ ê±´ê°•ì— ì¢‹ìŠµë‹ˆë‹¤. í•˜ë£¨ 30ë¶„ ê±·ê¸°ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤.",
                "expected_relevant": False
            },
            # ë¶€ë¶„ì  ê´€ë ¨ì„±
            {
                "query": "ì›¹ ê°œë°œ í”„ë ˆì„ì›Œí¬",
                "context": "ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œì—ëŠ” ë‹¤ì–‘í•œ ë„êµ¬ê°€ ìˆìŠµë‹ˆë‹¤. í”„ë ˆì„ì›Œí¬ëŠ” ê°œë°œ ìƒì‚°ì„±ì„ ë†’ì—¬ì¤ë‹ˆë‹¤.",
                "expected_relevant": True  # ë¶€ë¶„ì  ê´€ë ¨ì„±
            },
            {
                "query": "í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤",
                "context": "IT ì¸í”„ë¼ ê´€ë¦¬ëŠ” ë³µì¡í•©ë‹ˆë‹¤. ì„œë²„ ìš´ì˜ì—ëŠ” ë§ì€ ë¦¬ì†ŒìŠ¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.",
                "expected_relevant": True  # ë¶€ë¶„ì  ê´€ë ¨ì„±
            },
        ]
        return cases

    def generate_judge_cases(self) -> List[Dict]:
        """LLM-as-Judge í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤"""
        cases = [
            # ì¢‹ì€ ì‘ë‹µ
            {
                "query": "Pythonì—ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ëŠ” ë°©ë²•ì€?",
                "response": """Pythonì—ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ëŠ” ë°©ë²•ì€ í¬ê²Œ ë‘ ê°€ì§€ê°€ ìˆìŠµë‹ˆë‹¤:

1. **sort() ë©”ì„œë“œ**: ì›ë³¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ì •ë ¬
```python
my_list = [3, 1, 4, 1, 5]
my_list.sort()  # [1, 1, 3, 4, 5]
```

2. **sorted() í•¨ìˆ˜**: ìƒˆë¡œìš´ ì •ë ¬ëœ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
```python
my_list = [3, 1, 4, 1, 5]
new_list = sorted(my_list)  # [1, 1, 3, 4, 5]
```

ì—­ìˆœ ì •ë ¬ì€ `reverse=True` ì˜µì…˜ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.""",
                "expected_score_min": 4.0
            },
            # ë³´í†µ ì‘ë‹µ
            {
                "query": "ë¨¸ì‹ ëŸ¬ë‹ì´ ë­ì•¼?",
                "response": "ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ëŠ” AIì…ë‹ˆë‹¤.",
                "expected_score_min": 2.5
            },
            # ë‚˜ìœ ì‘ë‹µ
            {
                "query": "React ìƒíƒœ ê´€ë¦¬ ë°©ë²•",
                "response": "ì˜ ëª¨ë¥´ê² ì–´ìš”.",
                "expected_score_min": 1.0
            },
        ]
        return cases

    async def test_semantic_router(self) -> Dict[str, Any]:
        """ì‹œë§¨í‹± ë¼ìš°í„° í…ŒìŠ¤íŠ¸"""
        print("\nğŸ“Š ì‹œë§¨í‹± ë¼ìš°í„° í…ŒìŠ¤íŠ¸")
        print("-" * 50)

        test_cases = self.generate_test_cases()
        correct = 0
        total = len(test_cases)
        results_detail = []
        latencies = []

        for tc in test_cases:
            start = time.time()
            intent, score, config = await self.semantic_router.route_async(tc.query)
            latency = (time.time() - start) * 1000

            is_correct = intent == tc.expected_intent
            if is_correct:
                correct += 1

            latencies.append(latency)
            results_detail.append({
                "query": tc.query,
                "expected": tc.expected_intent.value,
                "predicted": intent.value,
                "correct": is_correct,
                "latency_ms": latency
            })

            status = "âœ“" if is_correct else "âœ—"
            print(f"   {status} [{tc.expected_intent.value}â†’{intent.value}] {tc.query[:30]}...")

        accuracy = correct / total
        avg_latency = sum(latencies) / len(latencies)

        print(f"\n   ì •í™•ë„: {accuracy:.1%} ({correct}/{total})")
        print(f"   í‰ê·  ë ˆì´í„´ì‹œ: {avg_latency:.1f}ms")

        return {
            "accuracy": accuracy,
            "correct": correct,
            "total": total,
            "avg_latency_ms": avg_latency,
            "details": results_detail
        }

    async def test_self_rag(self) -> Dict[str, Any]:
        """Self-RAG ê´€ë ¨ì„± í…ŒìŠ¤íŠ¸"""
        print("\nğŸ“Š Self-RAG ê´€ë ¨ì„± í…ŒìŠ¤íŠ¸")
        print("-" * 50)

        test_cases = self.generate_relevance_cases()
        correct = 0
        total = len(test_cases)
        results_detail = []
        latencies = []

        for tc in test_cases:
            start = time.time()
            result = await self.self_rag.check_relevance(tc["query"], tc["context"])
            latency = (time.time() - start) * 1000

            is_correct = result.is_relevant == tc["expected_relevant"]
            if is_correct:
                correct += 1

            latencies.append(latency)
            results_detail.append({
                "query": tc["query"],
                "expected_relevant": tc["expected_relevant"],
                "predicted_relevant": result.is_relevant,
                "confidence": result.confidence,
                "correct": is_correct,
                "latency_ms": latency
            })

            status = "âœ“" if is_correct else "âœ—"
            exp_str = "ê´€ë ¨" if tc["expected_relevant"] else "ë¬´ê´€"
            pred_str = "ê´€ë ¨" if result.is_relevant else "ë¬´ê´€"
            print(f"   {status} [{exp_str}â†’{pred_str}] (conf={result.confidence:.2f}) {tc['query'][:25]}...")

        accuracy = correct / total
        avg_latency = sum(latencies) / len(latencies)

        print(f"\n   ì •í™•ë„: {accuracy:.1%} ({correct}/{total})")
        print(f"   í‰ê·  ë ˆì´í„´ì‹œ: {avg_latency:.1f}ms")

        return {
            "accuracy": accuracy,
            "correct": correct,
            "total": total,
            "avg_latency_ms": avg_latency,
            "details": results_detail
        }

    async def test_llm_judge(self) -> Dict[str, Any]:
        """LLM-as-Judge í…ŒìŠ¤íŠ¸"""
        print("\nğŸ“Š LLM-as-Judge í‰ê°€ í…ŒìŠ¤íŠ¸")
        print("-" * 50)

        test_cases = self.generate_judge_cases()
        scores = []
        results_detail = []
        latencies = []

        for tc in test_cases:
            start = time.time()
            judge_score = await self.llm_judge.evaluate(
                query=tc["query"],
                response=tc["response"]
            )
            latency = (time.time() - start) * 1000

            meets_expectation = judge_score.overall >= tc["expected_score_min"]
            latencies.append(latency)
            scores.append(judge_score.overall)

            results_detail.append({
                "query": tc["query"],
                "response": tc["response"][:50] + "...",
                "accuracy": judge_score.accuracy,
                "helpfulness": judge_score.helpfulness,
                "personalization": judge_score.personalization,
                "friendliness": judge_score.friendliness,
                "overall": judge_score.overall,
                "expected_min": tc["expected_score_min"],
                "meets_expectation": meets_expectation,
                "latency_ms": latency
            })

            status = "âœ“" if meets_expectation else "âœ—"
            print(f"   {status} Overall: {judge_score.overall:.2f}/5 (ê¸°ëŒ€: >={tc['expected_score_min']:.1f})")
            print(f"      ì •í™•ì„±:{judge_score.accuracy} ë„ì›€:{judge_score.helpfulness} "
                  f"ê°œì¸í™”:{judge_score.personalization} ì¹œì ˆ:{judge_score.friendliness}")

        avg_score = sum(scores) / len(scores)
        avg_latency = sum(latencies) / len(latencies)

        print(f"\n   í‰ê·  ì ìˆ˜: {avg_score:.2f}/5")
        print(f"   í‰ê·  ë ˆì´í„´ì‹œ: {avg_latency:.1f}ms")

        return {
            "avg_score": avg_score,
            "scores": scores,
            "avg_latency_ms": avg_latency,
            "details": results_detail
        }

    async def run_all_tests(self) -> Dict[str, Any]:
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        results = {}

        # ì‹œë§¨í‹± ë¼ìš°í„° í…ŒìŠ¤íŠ¸
        results["semantic_router"] = await self.test_semantic_router()

        # Self-RAG í…ŒìŠ¤íŠ¸
        results["self_rag"] = await self.test_self_rag()

        # LLM-as-Judge í…ŒìŠ¤íŠ¸
        results["llm_judge"] = await self.test_llm_judge()

        return results


async def main():
    print("=" * 70)
    print("LLM-as-Judge ë²¤ì¹˜ë§ˆí¬")
    print("ê°œì¸í™” í–¥ìƒ ëª¨ë“ˆ ì„±ëŠ¥ í‰ê°€")
    print("=" * 70)

    benchmark = LLMJudgeBenchmark()
    await benchmark.initialize()

    results = await benchmark.run_all_tests()

    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 70)
    print("ğŸ“ˆ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½")
    print("=" * 70)

    print("\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("â”‚ Component               â”‚ Accuracy     â”‚ Latency (ms) â”‚")
    print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")

    sr = results["semantic_router"]
    print(f"â”‚ Semantic Router         â”‚ {sr['accuracy']:>10.1%} â”‚ {sr['avg_latency_ms']:>10.1f} â”‚")

    srag = results["self_rag"]
    print(f"â”‚ Self-RAG                â”‚ {srag['accuracy']:>10.1%} â”‚ {srag['avg_latency_ms']:>10.1f} â”‚")

    judge = results["llm_judge"]
    print(f"â”‚ LLM-as-Judge            â”‚ {judge['avg_score']/5:>10.1%} â”‚ {judge['avg_latency_ms']:>10.1f} â”‚")

    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")

    # ê²°ê³¼ ì €ì¥
    output = {
        "benchmark": "LLM-as-Judge",
        "timestamp": datetime.now().isoformat(),
        "results": results
    }

    output_path = "/tmp/llm_judge_benchmark_results.json"
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2, default=str)

    print(f"\nğŸ“ ê²°ê³¼ ì €ì¥: {output_path}")


if __name__ == "__main__":
    asyncio.run(main())
