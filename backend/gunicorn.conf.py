"""
Gunicorn í”„ë¡œë•ì…˜ ì„¤ì •
200ëª… ì´ìƒ ë™ì‹œ ì‚¬ìš©ì ì§€ì›ì„ ìœ„í•œ ìµœì í™”
"""
import multiprocessing
import os

# GPU ì„¤ì • (5ë²ˆ GPU ì‚¬ìš©)
os.environ["CUDA_VISIBLE_DEVICES"] = "5"

# ì„œë²„ ë°”ì¸ë”©
bind = os.getenv("GUNICORN_BIND", "0.0.0.0:8600")

# ì›Œì»¤ ì„¤ì •
# í…ŒìŠ¤íŠ¸ í™˜ê²½: 4ê°œ ê³ ì • (ìš´ì˜ í™˜ê²½ì—ì„œëŠ” CPU ì½”ì–´ ìˆ˜ * 2 + 1 ê¶Œì¥)
workers = int(os.getenv("GUNICORN_WORKERS", 4))
worker_class = "uvicorn.workers.UvicornWorker"

# ì›Œì»¤ íƒ€ì„ì•„ì›ƒ (LLM ì‘ë‹µ ëŒ€ê¸° ì‹œê°„ ê³ ë ¤)
timeout = 300  # 5ë¶„
graceful_timeout = 60
keepalive = 5

# ë™ì‹œ ì—°ê²° ìˆ˜ (ì›Œì»¤ë‹¹)
worker_connections = 1000

# ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€: ìš”ì²­ Nê°œ ì²˜ë¦¬ í›„ ì›Œì»¤ ì¬ì‹œì‘
max_requests = 1000
max_requests_jitter = 50

# í”„ë¦¬í¬í¬ (ì›Œì»¤ ì‚¬ì „ ìƒì„±)
preload_app = True

# ë¡œê¹…
loglevel = os.getenv("GUNICORN_LOG_LEVEL", "info")
accesslog = "access.log"
errorlog = "error.log"
access_log_format = '%(h)s %(l)s %(u)s %(t)s "%(r)s" %(s)s %(b)s "%(f)s" "%(a)s" %(D)s'

# í”„ë¡œì„¸ìŠ¤ ì´ë¦„
proc_name = "llm-chatbot"

# ì„ì‹œ íŒŒì¼ ë””ë ‰í† ë¦¬ (ë©”ëª¨ë¦¬ ê¸°ë°˜)
worker_tmp_dir = "/dev/shm"

# ìš”ì²­ ì œí•œ
limit_request_line = 8190
limit_request_fields = 100
limit_request_field_size = 8190


def on_starting(server):
    """ì„œë²„ ì‹œì‘ ì‹œ í˜¸ì¶œ"""
    print(f"ğŸš€ Starting Gunicorn with {workers} workers...")


def on_exit(server):
    """ì„œë²„ ì¢…ë£Œ ì‹œ í˜¸ì¶œ"""
    print("ğŸ‘‹ Gunicorn shutdown complete")


def worker_int(worker):
    """ì›Œì»¤ ì¸í„°ëŸ½íŠ¸ ì‹œ í˜¸ì¶œ"""
    print(f"Worker {worker.pid} interrupted")


def worker_abort(worker):
    """ì›Œì»¤ ë¹„ì •ìƒ ì¢…ë£Œ ì‹œ í˜¸ì¶œ"""
    print(f"Worker {worker.pid} aborted")
