"""
V7.2 Multimodal Hierarchical RAG

V7.1 ëŒ€ë¹„ ë³€ê²½ì :
- Azure OCR â†’ Upstage Document OCR
- ì´ë¯¸ì§€ ì²˜ë¦¬ ì†ë„ 9ë°° í–¥ìƒ (4524ms â†’ 486ms)
- ë™ì¼í•œ ì‹ ë¢°ë„ ìœ ì§€ (0.95)

Collection: mh_rag_finance_v7_2
"""

import asyncio
from pathlib import Path
from typing import Optional, AsyncIterator
import os
import sys

sys.path.insert(0, str(Path(__file__).parent.parent))

from config import get_settings, Settings

from rag.parsers import (
    UpstageDocumentParser,
    OpenAIImageCaptioner,
    BatchImageCaptioner,
    ParsedDocument,
    CaptionResult,
)
from rag.parsers.hybrid_image_processor_v72 import HybridImageProcessorV72, BatchHybridProcessorV72
from rag.chunkers import (
    HierarchicalChunker,
    ParentChunk,
    ChildChunk,
)
from rag.embeddings import (
    OpenAIEmbeddingService,
    SparseEmbeddingService,
    MultimodalEmbeddingService,
)
from rag.vectorstore import (
    QdrantVectorStore,
    HybridSearchConfig,
)
from rag.retriever import (
    HierarchicalRetriever,
    RetrievalConfig,
    RetrievalResult,
    EnhancedHierarchicalRetriever,
    EnhancedRetrievalConfig,
    EnhancedRetrievalResult,
)
from rag.chain import (
    RAGChain,
    RAGResponse,
    ChatMessage,
)


# V7.2 Collection ì´ë¦„
V72_COLLECTION_NAME = "mh_rag_finance_v7_2"


class MultimodalHierarchicalRAGV72:
    """V7.2 ë©€í‹°ëª¨ë‹¬ ê³„ì¸µì  RAG ì‹œìŠ¤í…œ

    V7.1 ëŒ€ë¹„ ë³€ê²½ì :
    - Upstage Document OCR ì‚¬ìš© (Azure ëŒ€ì‹ )
    - ì´ë¯¸ì§€ ì²˜ë¦¬ ì†ë„ 9ë°° í–¥ìƒ
    """

    def __init__(
        self,
        settings: Optional[Settings] = None,
        collection_name: str = V72_COLLECTION_NAME,
    ):
        """
        Args:
            settings: ì„¤ì • ê°ì²´ (ì—†ìœ¼ë©´ í™˜ê²½ ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
            collection_name: Qdrant Collection ì´ë¦„
        """
        self.settings = settings or get_settings()
        self.collection_name = collection_name
        self._initialized = False

        self._parser: Optional[UpstageDocumentParser] = None
        self._captioner: Optional[BatchImageCaptioner] = None
        self._hybrid_processor: Optional[BatchHybridProcessorV72] = None
        self._use_hybrid_image: bool = True
        self._chunker: Optional[HierarchicalChunker] = None
        self._embedding_service: Optional[MultimodalEmbeddingService] = None
        self._vector_store: Optional[QdrantVectorStore] = None
        self._retriever: Optional[HierarchicalRetriever] = None
        self._enhanced_retriever: Optional[EnhancedHierarchicalRetriever] = None
        self._chain: Optional[RAGChain] = None
        self._use_enhanced: bool = True

    async def initialize(self) -> None:
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        if self._initialized:
            return

        print("=" * 60)
        print("V7.2 Multimodal Hierarchical RAG ì´ˆê¸°í™”")
        print("  - OCR: Upstage Document OCR (Azure ëŒ€ì²´)")
        print("  - VLM: GPT-4o")
        print(f"  - Collection: {self.collection_name}")
        print("=" * 60)

        self._parser = UpstageDocumentParser(
            api_key=self.settings.upstage_api_key,
        )

        image_captioner = OpenAIImageCaptioner(
            api_key=self.settings.openai_api_key,
            model=self.settings.vlm_model,
        )
        self._captioner = BatchImageCaptioner(captioner=image_captioner)

        # V7.2: Upstage Document OCR + VLM í•˜ì´ë¸Œë¦¬ë“œ í”„ë¡œì„¸ì„œ
        hybrid_processor = HybridImageProcessorV72(
            openai_api_key=self.settings.openai_api_key,
            upstage_api_key=self.settings.upstage_api_key,
            vlm_model=self.settings.vlm_model,
        )
        if hybrid_processor.initialize():
            self._hybrid_processor = BatchHybridProcessorV72(
                processor=hybrid_processor,
                max_concurrent=5,  # Upstage OCRì´ ë¹ ë¥´ë¯€ë¡œ ë™ì‹œì„± ì¦ê°€
            )
            print("  âœ“ Upstage Document OCR í•˜ì´ë¸Œë¦¬ë“œ ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ í™œì„±í™” (V7.2)")
        else:
            self._use_hybrid_image = False
            print("  âœ— Upstage OCR ë¯¸ì„¤ì •, ê¸°ë³¸ VLM ìº¡ì…”ë„ˆ ì‚¬ìš©")

        self._chunker = HierarchicalChunker(
            parent_chunk_size=self.settings.parent_chunk_size,
            child_chunk_size=self.settings.child_chunk_size,
            chunk_overlap=self.settings.chunk_overlap,
        )

        dense_service = OpenAIEmbeddingService(
            api_key=self.settings.openai_api_key,
            model=self.settings.embedding_model,
        )
        sparse_service = SparseEmbeddingService()
        self._embedding_service = MultimodalEmbeddingService(
            dense_service=dense_service,
            sparse_service=sparse_service,
        )

        # V7.2 ì „ìš© Collection ì‚¬ìš©
        self._vector_store = QdrantVectorStore(
            host=self.settings.qdrant_host,
            port=self.settings.qdrant_port,
            api_key=self.settings.qdrant_api_key,
            collection_name=self.collection_name,
        )
        await self._vector_store.initialize()

        # ê¸°ë³¸ Retriever
        self._retriever = HierarchicalRetriever(
            vector_store=self._vector_store,
            embedding_service=self._embedding_service,
            config=RetrievalConfig(
                top_k=8,
                use_hybrid=False,
                expand_to_parent=True,
                rerank=True,
                rerank_top_k=25,
            ),
        )

        # Enhanced Retriever V7 (V7.1ê³¼ ë™ì¼í•œ ì„¤ì •)
        self._enhanced_retriever = EnhancedHierarchicalRetriever(
            vector_store=self._vector_store,
            embedding_service=self._embedding_service,
            api_key=self.settings.openai_api_key,
            config=EnhancedRetrievalConfig(
                top_k=20,
                rerank_top_k=60,
                expand_to_parent=True,
                rerank=True,
                enable_query_expansion=True,
                enable_multi_query=False,
                enable_adaptive_search=True,
                enable_rrf=True,
                enable_hyde=False,
                num_sub_queries=3,
                rrf_k=60,
                rrf_dense_weight=1.5,
                rrf_sparse_weight=0.5,
                enable_table_adaptive=True,
            ),
        )
        print("  âœ“ Enhanced Retriever V7 í™œì„±í™”")

        self._chain = RAGChain(
            retriever=self._retriever,
            api_key=self.settings.openai_api_key,
            model=self.settings.llm_model,
        )

        self._initialized = True
        print(f"\nâœ“ V7.2 Multimodal Hierarchical RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")

    async def ingest_document(
        self,
        file_path: str | Path,
        caption_images: bool = True,
    ) -> dict:
        """ë¬¸ì„œë¥¼ ì¸ë±ì‹±"""
        if not self._initialized:
            await self.initialize()

        file_path = Path(file_path)
        print(f"\nðŸ“„ ë¬¸ì„œ íŒŒì‹± ì¤‘: {file_path.name}")

        parsed_doc = await self._parser.parse(file_path)
        print(f"  - ì´ {len(parsed_doc.elements)}ê°œ ìš”ì†Œ, {parsed_doc.total_pages}íŽ˜ì´ì§€")

        caption_results = []
        hybrid_results = []
        if caption_images:
            images = parsed_doc.get_images_and_charts()
            if images:
                print(f"ðŸ–¼ï¸ ì´ë¯¸ì§€/ì°¨íŠ¸ ì²˜ë¦¬ ì¤‘: {len(images)}ê°œ")

                # V7.2: Upstage Document OCR + VLM í•˜ì´ë¸Œë¦¬ë“œ ì²˜ë¦¬
                if self._use_hybrid_image and self._hybrid_processor:
                    print("  - V7.2 í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ (Upstage Document OCR + VLM)")
                    hybrid_results = await self._hybrid_processor.process_elements(
                        elements=parsed_doc.elements,
                    )
                    # í•˜ì´ë¸Œë¦¬ë“œ ê²°ê³¼ë¥¼ CaptionResultë¡œ ë³€í™˜
                    for hr in hybrid_results:
                        caption_results.append(CaptionResult(
                            element_id=hr.element_id,
                            original_content="",
                            caption=hr.combined_content,
                            summary=hr.vlm_summary,
                            key_values={},
                            metadata=hr.metadata,
                        ))
                    print(f"  - {len(hybrid_results)}ê°œ V7.2 í•˜ì´ë¸Œë¦¬ë“œ ì²˜ë¦¬ ì™„ë£Œ")
                else:
                    # ê¸°ë³¸ VLM ìº¡ì…”ë„ˆ
                    print("  - VLM ìº¡ì…”ë„ˆ ëª¨ë“œ")
                    caption_results = await self._captioner.caption_elements(
                        elements=parsed_doc.elements,
                    )
                    print(f"  - {len(caption_results)}ê°œ ìº¡ì…”ë‹ ì™„ë£Œ")

        print(f"ðŸ“ ê³„ì¸µì  ì²­í‚¹ ì¤‘...")
        parent_chunks, child_chunks = self._chunker.chunk_document(
            document=parsed_doc,
            caption_results=caption_results,
        )
        print(f"  - ë¶€ëª¨ ì²­í¬: {len(parent_chunks)}ê°œ")
        print(f"  - ìžì‹ ì²­í¬: {len(child_chunks)}ê°œ")

        print(f"ðŸ”¢ ìž„ë² ë”© ìƒì„± ì¤‘...")
        dense_embeddings, sparse_embeddings = await self._embedding_service.embed_chunks(
            parent_chunks=parent_chunks,
            child_chunks=child_chunks,
        )
        print(f"  - {len(dense_embeddings)}ê°œ ìž„ë² ë”© ìƒì„± ì™„ë£Œ")

        print(f"ðŸ’¾ ë²¡í„° DBì— ì €ìž¥ ì¤‘...")
        await self._vector_store.add_chunks(
            parent_chunks=parent_chunks,
            child_chunks=child_chunks,
            dense_embeddings=dense_embeddings,
            sparse_embeddings=sparse_embeddings,
        )
        print(f"âœ“ ë¬¸ì„œ ì¸ë±ì‹± ì™„ë£Œ: {file_path.name}")

        return {
            "filename": file_path.name,
            "total_pages": parsed_doc.total_pages,
            "total_elements": len(parsed_doc.elements),
            "parent_chunks": len(parent_chunks),
            "child_chunks": len(child_chunks),
            "captioned_images": len(caption_results),
            "version": "v7.2",
            "ocr_provider": "upstage_document_ocr",
        }

    async def ingest_documents(
        self,
        file_paths: list[str | Path],
        caption_images: bool = True,
    ) -> list[dict]:
        """ì—¬ëŸ¬ ë¬¸ì„œë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì¸ë±ì‹±"""
        results = []
        for file_path in file_paths:
            try:
                result = await self.ingest_document(file_path, caption_images)
                results.append(result)
            except Exception as e:
                results.append({
                    "filename": str(file_path),
                    "error": str(e),
                })
        return results

    async def chat(
        self,
        query: str,
        conversation_history: Optional[list[ChatMessage]] = None,
        use_enhanced: Optional[bool] = None,
    ) -> RAGResponse:
        """RAG ê¸°ë°˜ ì±„íŒ…"""
        if not self._initialized:
            await self.initialize()

        should_use_enhanced = use_enhanced if use_enhanced is not None else self._use_enhanced

        if should_use_enhanced and self._enhanced_retriever:
            return await self._chat_enhanced(query, conversation_history)
        else:
            return await self._chain.chat(
                query=query,
                conversation_history=conversation_history,
            )

    async def _chat_enhanced(
        self,
        query: str,
        conversation_history: Optional[list[ChatMessage]] = None,
    ) -> RAGResponse:
        """Enhanced Retrieverë¥¼ ì‚¬ìš©í•œ ì±„íŒ…"""
        enhanced_result = await self._enhanced_retriever.retrieve(query)

        answer = await self._generate_answer(
            query=query,
            context=enhanced_result.context,
            conversation_history=conversation_history,
        )

        return RAGResponse(
            answer=answer,
            sources=enhanced_result.sources,
            retrieval_result=None,
            metadata={
                "model": self.settings.llm_model,
                "enhanced": True,
                "version": "v7.2",
                "ocr_provider": "upstage_document_ocr",
                "enhancements": enhanced_result.metadata.get("enhancements_applied", []),
                "keywords": enhanced_result.enhanced_query.keywords,
            },
        )

    async def _generate_answer(
        self,
        query: str,
        context: str,
        conversation_history: Optional[list[ChatMessage]] = None,
    ) -> str:
        """LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±"""
        import httpx

        system_prompt = """ë‹¹ì‹ ì€ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ AI ì–´ì‹œìŠ¤í„´íŠ¸ìž…ë‹ˆë‹¤.
ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ë‹¤ìŒ ì§€ì¹¨ì„ ë”°ë¼ì£¼ì„¸ìš”:
1. ì»¨í…ìŠ¤íŠ¸ì— ìžˆëŠ” ì •ë³´ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
2. ìˆ˜ì¹˜ë‚˜ ë°ì´í„°ë¥¼ ì¸ìš©í•  ë•ŒëŠ” ì •í™•í•˜ê²Œ ì¸ìš©í•˜ì„¸ìš”.
3. ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ ì†”ì§í•˜ê²Œ "ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”.
4. ì°¨íŠ¸ë‚˜ ê·¸ëž˜í”„ ë¶„ì„ ë‚´ìš©ì´ í¬í•¨ë˜ì–´ ìžˆë‹¤ë©´ ê·¸ ë‚´ìš©ë„ í™œìš©í•˜ì„¸ìš”.
5. ë‹µë³€ ë§ˆì§€ë§‰ì— ì°¸ì¡°í•œ ì¶œì²˜(ë¬¸ì„œëª…, íŽ˜ì´ì§€)ë¥¼ ê°„ë‹¨ížˆ ì–¸ê¸‰í•´ì£¼ì„¸ìš”.

ë‹µë³€ì€ ëª…í™•í•˜ê³  êµ¬ì¡°í™”ë˜ì–´ì•¼ í•˜ë©°, í•„ìš”ì‹œ ë¶ˆë¦¿ í¬ì¸íŠ¸ë‚˜ ë²ˆí˜¸ ë§¤ê¸°ê¸°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."""

        messages = [{"role": "system", "content": system_prompt}]

        if conversation_history:
            for msg in conversation_history[-6:]:
                messages.append({"role": msg.role, "content": msg.content})

        user_message = f"""ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

## ì»¨í…ìŠ¤íŠ¸
{context}

## ì§ˆë¬¸
{query}"""

        messages.append({"role": "user", "content": user_message})

        headers = {
            "Authorization": f"Bearer {self.settings.openai_api_key}",
            "Content-Type": "application/json",
        }

        payload = {
            "model": self.settings.llm_model,
            "messages": messages,
            "temperature": 0.3,
            "max_completion_tokens": 2000,
        }

        async with httpx.AsyncClient(timeout=120.0) as client:
            response = await client.post(
                "https://api.openai.com/v1/chat/completions",
                headers=headers,
                json=payload,
            )

        if response.status_code != 200:
            raise Exception(f"OpenAI API ì˜¤ë¥˜: {response.status_code} - {response.text}")

        result = response.json()
        return result["choices"][0]["message"]["content"]

    async def search(
        self,
        query: str,
        top_k: int = 5,
        filter_source: Optional[str] = None,
    ) -> RetrievalResult:
        """ë¬¸ì„œ ê²€ìƒ‰ë§Œ ìˆ˜í–‰"""
        if not self._initialized:
            await self.initialize()

        return await self._retriever.retrieve(
            query=query,
            config=RetrievalConfig(top_k=top_k),
            filter_source=filter_source,
        )

    async def delete_collection(self) -> None:
        """Collection ì‚­ì œ (ìž¬ì¸ë±ì‹± ì „)"""
        if not self._initialized:
            await self.initialize()

        await self._vector_store.delete_collection()
        print(f"âœ“ Collection ì‚­ì œ ì™„ë£Œ: {self.collection_name}")

    async def close(self) -> None:
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self._vector_store:
            await self._vector_store.close()
        self._initialized = False


async def create_rag_system_v72(
    settings: Optional[Settings] = None,
    collection_name: str = V72_COLLECTION_NAME,
) -> MultimodalHierarchicalRAGV72:
    """V7.2 RAG ì‹œìŠ¤í…œ íŒ©í† ë¦¬ í•¨ìˆ˜"""
    rag = MultimodalHierarchicalRAGV72(settings=settings, collection_name=collection_name)
    await rag.initialize()
    return rag
