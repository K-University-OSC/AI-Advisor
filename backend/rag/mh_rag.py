"""
Multimodal Hierarchical RAG ë©”ì¸ ì¸í„°í˜ì´ìŠ¤
ëª¨ë“  ì»´í¬ë„ŒíŠ¸ë¥¼ í†µí•©í•˜ì—¬ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” API ì œê³µ
"""

import asyncio
from pathlib import Path
from typing import Optional, AsyncIterator
import os
import sys

# advisor/backendë¥¼ pathì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent))

from config import get_settings, Settings

from rag.parsers import (
    UpstageDocumentParser,
    OpenAIImageCaptioner,
    GeminiImageCaptioner,
    BatchImageCaptioner,
    ParsedDocument,
    CaptionResult,
)
from rag.parsers.hybrid_image_processor import HybridImageProcessor, BatchHybridProcessor
from rag.chunkers import (
    HierarchicalChunker,
    ParentChunk,
    ChildChunk,
)
from rag.embeddings import (
    OpenAIEmbeddingService,
    GeminiEmbeddingService,
    SparseEmbeddingService,
    MultimodalEmbeddingService,
)
from rag.vectorstore import (
    QdrantVectorStore,
    HybridSearchConfig,
)
from rag.retriever import (
    HierarchicalRetriever,
    RetrievalConfig,
    RetrievalResult,
    EnhancedHierarchicalRetriever,
    EnhancedRetrievalConfig,
    EnhancedRetrievalResult,
)
from rag.chain import (
    RAGChain,
    RAGResponse,
    ChatMessage,
)


class MultimodalHierarchicalRAG:
    """ë©€í‹°ëª¨ë‹¬ ê³„ì¸µì  RAG ì‹œìŠ¤í…œ"""

    def __init__(
        self,
        settings: Optional[Settings] = None,
    ):
        """
        Args:
            settings: ì„¤ì • ê°ì²´ (ì—†ìœ¼ë©´ í™˜ê²½ ë³€ìˆ˜ì—ì„œ ë¡œë“œ)
        """
        self.settings = settings or get_settings()
        self._initialized = False

        self._parser: Optional[UpstageDocumentParser] = None
        self._captioner: Optional[BatchImageCaptioner] = None
        self._hybrid_processor: Optional[BatchHybridProcessor] = None
        self._use_hybrid_image: bool = True  # í•˜ì´ë¸Œë¦¬ë“œ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‚¬ìš©
        self._chunker: Optional[HierarchicalChunker] = None
        self._embedding_service: Optional[MultimodalEmbeddingService] = None
        self._vector_store: Optional[QdrantVectorStore] = None
        self._retriever: Optional[HierarchicalRetriever] = None
        self._enhanced_retriever: Optional[EnhancedHierarchicalRetriever] = None
        self._chain: Optional[RAGChain] = None
        self._use_enhanced: bool = True  # Enhanced Retriever ì‚¬ìš© ì—¬ë¶€

    async def initialize(self) -> None:
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        if self._initialized:
            return

        self._parser = UpstageDocumentParser(
            api_key=self.settings.upstage_api_key,
        )

        # ì´ë¯¸ì§€ ìº¡ì…”ë„ˆ ì„ íƒ (VLM_PROVIDER í™˜ê²½ë³€ìˆ˜ì— ë”°ë¼)
        vlm_provider = os.getenv("VLM_PROVIDER", "openai").lower()
        if vlm_provider == "google":
            google_api_key = os.getenv("GOOGLE_API_KEY", "")
            image_captioner = GeminiImageCaptioner(
                api_key=google_api_key,
                model=self.settings.vlm_model or "gemini-3-flash-preview",
            )
            print(f"  - Gemini ì´ë¯¸ì§€ ìº¡ì…”ë„ˆ í™œì„±í™” ({self.settings.vlm_model or 'gemini-3-flash-preview'})")
        else:
            image_captioner = OpenAIImageCaptioner(
                api_key=self.settings.openai_api_key,
                model=self.settings.vlm_model,
            )
            print(f"  - OpenAI ì´ë¯¸ì§€ ìº¡ì…”ë„ˆ í™œì„±í™” ({self.settings.vlm_model})")
        self._captioner = BatchImageCaptioner(captioner=image_captioner)

        # í•˜ì´ë¸Œë¦¬ë“œ ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ (Azure OCR + VLM)
        hybrid_processor = HybridImageProcessor(
            openai_api_key=self.settings.openai_api_key,
            vlm_model=self.settings.vlm_model,
        )
        if hybrid_processor.initialize():
            self._hybrid_processor = BatchHybridProcessor(processor=hybrid_processor)
            print("  - Azure OCR í•˜ì´ë¸Œë¦¬ë“œ ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ í™œì„±í™”")
        else:
            self._use_hybrid_image = False
            print("  - Azure OCR ë¯¸ì„¤ì •, ê¸°ë³¸ VLM ìº¡ì…”ë„ˆ ì‚¬ìš©")

        self._chunker = HierarchicalChunker(
            parent_chunk_size=self.settings.parent_chunk_size,
            child_chunk_size=self.settings.child_chunk_size,
            chunk_overlap=self.settings.chunk_overlap,
        )

        # ì„ë² ë”© ì„œë¹„ìŠ¤ ì„ íƒ (EMBEDDING_PROVIDER í™˜ê²½ë³€ìˆ˜ì— ë”°ë¼)
        embedding_provider = os.getenv("EMBEDDING_PROVIDER", "openai").lower()
        if embedding_provider == "google":
            google_api_key = os.getenv("GOOGLE_API_KEY", "")
            dense_service = GeminiEmbeddingService(
                api_key=google_api_key,
                model=self.settings.embedding_model,
            )
            print(f"  - Gemini ì„ë² ë”© ì„œë¹„ìŠ¤ í™œì„±í™” ({self.settings.embedding_model})")
        else:
            dense_service = OpenAIEmbeddingService(
                api_key=self.settings.openai_api_key,
                model=self.settings.embedding_model,
            )
            print(f"  - OpenAI ì„ë² ë”© ì„œë¹„ìŠ¤ í™œì„±í™” ({self.settings.embedding_model})")
        sparse_service = SparseEmbeddingService()
        self._embedding_service = MultimodalEmbeddingService(
            dense_service=dense_service,
            sparse_service=sparse_service,
        )

        self._vector_store = QdrantVectorStore(
            host=self.settings.qdrant_host,
            port=self.settings.qdrant_port,
            api_key=self.settings.qdrant_api_key,
            collection_name=self.settings.qdrant_collection_name,
        )
        await self._vector_store.initialize()

        # ê¸°ë³¸ Retriever (í•˜ìœ„ í˜¸í™˜ì„±)
        self._retriever = HierarchicalRetriever(
            vector_store=self._vector_store,
            embedding_service=self._embedding_service,
            config=RetrievalConfig(
                top_k=8,
                use_hybrid=False,  # BM25 ë¹„í™œì„±í™” (í•œêµ­ì–´ RAG ì„±ëŠ¥ í–¥ìƒ)
                expand_to_parent=True,
                rerank=True,  # BGE Reranker í™œì„±í™”
                rerank_top_k=25,  # ë¦¬ë­í‚¹ ìµœì í™”
            ),
        )

        # Enhanced Retriever V7 (V6 + Query Expansion ë‹¤ì–‘í™” + Fallback ê²€ìƒ‰ + top_k ì¦ê°€)
        # V7 ê°œì„ ì‚¬í•­:
        # - Query Expansion: 3ê°œ ì´ìƒ ë‹¤ì–‘í•œ ê´€ì ì˜ ëŒ€ì•ˆ ì¿¼ë¦¬ ìƒì„±
        # - Fallback Search: ê²€ìƒ‰ ê²°ê³¼ ë¶€ì¡±/ë‚®ì€ ì ìˆ˜ ì‹œ ëŒ€ì•ˆ ì¿¼ë¦¬ë¡œ ì¬ê²€ìƒ‰
        # - top_k ì¦ê°€: 15 â†’ 20 (ë” ë§ì€ í›„ë³´ í™•ë³´)
        self._enhanced_retriever = EnhancedHierarchicalRetriever(
            vector_store=self._vector_store,
            embedding_service=self._embedding_service,
            api_key=self.settings.openai_api_key,
            config=EnhancedRetrievalConfig(
                top_k=20,  # 15â†’20: ë” ë§ì€ í›„ë³´ í™•ë³´ (V7)
                rerank_top_k=60,  # ë¦¬ë­í‚¹ í›„ë³´ ìœ ì§€
                expand_to_parent=True,
                rerank=True,
                # í•µì‹¬ ê¸°ëŠ¥ í™œì„±í™”
                enable_query_expansion=True,  # V7: ë‹¤ì–‘í™”ëœ ì¿¼ë¦¬ í™•ì¥
                enable_multi_query=False,  # RRFì™€ í•¨ê»˜ ì‚¬ìš© ì‹œ ë¹„í™œì„±í™”
                enable_adaptive_search=True,
                enable_rrf=True,  # RRF í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í™œì„±í™”
                enable_hyde=False,  # HyDEëŠ” ì‹¤í—˜ì 
                num_sub_queries=3,  # V7: 3ê°œ ëŒ€ì•ˆ ì¿¼ë¦¬ ìƒì„±
                # RRF íŒŒë¼ë¯¸í„°
                rrf_k=60,
                rrf_dense_weight=1.5,
                rrf_sparse_weight=0.5,
                # í…Œì´ë¸”/ì´ë¯¸ì§€ ì¿¼ë¦¬ ì ì‘í˜• ê²€ìƒ‰
                enable_table_adaptive=True,
            ),
        )
        print("  - Enhanced Retriever V7 í™œì„±í™” (top_k=20, fallback_search, query_expansion_v2)")

        self._chain = RAGChain(
            retriever=self._retriever,
            api_key=self.settings.openai_api_key,
            model=self.settings.llm_model,
        )

        self._initialized = True
        print(f"âœ“ Multimodal Hierarchical RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")

    async def ingest_document(
        self,
        file_path: str | Path,
        caption_images: bool = True,
    ) -> dict:
        """
        ë¬¸ì„œë¥¼ ì¸ë±ì‹±

        Args:
            file_path: ë¬¸ì„œ íŒŒì¼ ê²½ë¡œ
            caption_images: ì´ë¯¸ì§€/ì°¨íŠ¸ ìº¡ì…”ë‹ ì—¬ë¶€

        Returns:
            ì¸ë±ì‹± ê²°ê³¼ ì •ë³´
        """
        if not self._initialized:
            await self.initialize()

        file_path = Path(file_path)
        print(f"ğŸ“„ ë¬¸ì„œ íŒŒì‹± ì¤‘: {file_path.name}")

        parsed_doc = await self._parser.parse(file_path)
        print(f"  - ì´ {len(parsed_doc.elements)}ê°œ ìš”ì†Œ, {parsed_doc.total_pages}í˜ì´ì§€")

        caption_results = []
        hybrid_results = []
        if caption_images:
            images = parsed_doc.get_images_and_charts()
            if images:
                print(f"ğŸ–¼ï¸ ì´ë¯¸ì§€/ì°¨íŠ¸ ì²˜ë¦¬ ì¤‘: {len(images)}ê°œ")

                # í•˜ì´ë¸Œë¦¬ë“œ ì²˜ë¦¬ (Azure OCR + VLM)
                if self._use_hybrid_image and self._hybrid_processor:
                    print("  - í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ (Azure OCR + VLM)")
                    hybrid_results = await self._hybrid_processor.process_elements(
                        elements=parsed_doc.elements,
                    )
                    # í•˜ì´ë¸Œë¦¬ë“œ ê²°ê³¼ë¥¼ CaptionResultë¡œ ë³€í™˜
                    for hr in hybrid_results:
                        caption_results.append(CaptionResult(
                            element_id=hr.element_id,
                            original_content="",
                            caption=hr.combined_content,
                            summary=hr.vlm_summary,
                            key_values={},
                            metadata=hr.metadata,
                        ))
                    print(f"  - {len(hybrid_results)}ê°œ í•˜ì´ë¸Œë¦¬ë“œ ì²˜ë¦¬ ì™„ë£Œ")
                else:
                    # ê¸°ë³¸ VLM ìº¡ì…”ë„ˆ
                    print("  - VLM ìº¡ì…”ë„ˆ ëª¨ë“œ")
                    caption_results = await self._captioner.caption_elements(
                        elements=parsed_doc.elements,
                    )
                    print(f"  - {len(caption_results)}ê°œ ìº¡ì…”ë‹ ì™„ë£Œ")

        print(f"ğŸ“ ê³„ì¸µì  ì²­í‚¹ ì¤‘...")
        parent_chunks, child_chunks = self._chunker.chunk_document(
            document=parsed_doc,
            caption_results=caption_results,
        )
        print(f"  - ë¶€ëª¨ ì²­í¬: {len(parent_chunks)}ê°œ")
        print(f"  - ìì‹ ì²­í¬: {len(child_chunks)}ê°œ")

        print(f"ğŸ”¢ ì„ë² ë”© ìƒì„± ì¤‘...")
        dense_embeddings, sparse_embeddings = await self._embedding_service.embed_chunks(
            parent_chunks=parent_chunks,
            child_chunks=child_chunks,
        )
        print(f"  - {len(dense_embeddings)}ê°œ ì„ë² ë”© ìƒì„± ì™„ë£Œ")

        print(f"ğŸ’¾ ë²¡í„° DBì— ì €ì¥ ì¤‘...")
        await self._vector_store.add_chunks(
            parent_chunks=parent_chunks,
            child_chunks=child_chunks,
            dense_embeddings=dense_embeddings,
            sparse_embeddings=sparse_embeddings,
        )
        print(f"âœ“ ë¬¸ì„œ ì¸ë±ì‹± ì™„ë£Œ: {file_path.name}")

        return {
            "filename": file_path.name,
            "total_pages": parsed_doc.total_pages,
            "total_elements": len(parsed_doc.elements),
            "parent_chunks": len(parent_chunks),
            "child_chunks": len(child_chunks),
            "captioned_images": len(caption_results),
        }

    async def ingest_documents(
        self,
        file_paths: list[str | Path],
        caption_images: bool = True,
    ) -> list[dict]:
        """
        ì—¬ëŸ¬ ë¬¸ì„œë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì¸ë±ì‹±

        Args:
            file_paths: ë¬¸ì„œ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            caption_images: ì´ë¯¸ì§€/ì°¨íŠ¸ ìº¡ì…”ë‹ ì—¬ë¶€

        Returns:
            ê° ë¬¸ì„œì˜ ì¸ë±ì‹± ê²°ê³¼
        """
        results = []
        for file_path in file_paths:
            try:
                result = await self.ingest_document(file_path, caption_images)
                results.append(result)
            except Exception as e:
                results.append({
                    "filename": str(file_path),
                    "error": str(e),
                })
        return results

    async def chat(
        self,
        query: str,
        conversation_history: Optional[list[ChatMessage]] = None,
        use_enhanced: Optional[bool] = None,
    ) -> RAGResponse:
        """
        RAG ê¸°ë°˜ ì±„íŒ…

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            conversation_history: ì´ì „ ëŒ€í™” ê¸°ë¡
            use_enhanced: Enhanced Retriever ì‚¬ìš© ì—¬ë¶€ (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)

        Returns:
            RAG ì‘ë‹µ
        """
        if not self._initialized:
            await self.initialize()

        # Enhanced Retriever ì‚¬ìš© ì—¬ë¶€ ê²°ì •
        should_use_enhanced = use_enhanced if use_enhanced is not None else self._use_enhanced

        if should_use_enhanced and self._enhanced_retriever:
            return await self._chat_enhanced(query, conversation_history)
        else:
            return await self._chain.chat(
                query=query,
                conversation_history=conversation_history,
            )

    async def _chat_enhanced(
        self,
        query: str,
        conversation_history: Optional[list[ChatMessage]] = None,
    ) -> RAGResponse:
        """Enhanced Retrieverë¥¼ ì‚¬ìš©í•œ ì±„íŒ…"""
        # Enhanced Retrieval ìˆ˜í–‰
        enhanced_result = await self._enhanced_retriever.retrieve(query)

        # LLM ì‘ë‹µ ìƒì„±
        answer = await self._generate_answer(
            query=query,
            context=enhanced_result.context,
            conversation_history=conversation_history,
        )

        return RAGResponse(
            answer=answer,
            sources=enhanced_result.sources,
            retrieval_result=None,  # Enhanced ê²°ê³¼ëŠ” ë³„ë„ íƒ€ì…
            metadata={
                "model": self.settings.llm_model,
                "enhanced": True,
                "enhancements": enhanced_result.metadata.get("enhancements_applied", []),
                "keywords": enhanced_result.enhanced_query.keywords,
            },
        )

    async def _generate_answer(
        self,
        query: str,
        context: str,
        conversation_history: Optional[list[ChatMessage]] = None,
    ) -> str:
        """LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±"""
        import httpx

        system_prompt = """ë‹¹ì‹ ì€ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ë‹¤ìŒ ì§€ì¹¨ì„ ë”°ë¼ì£¼ì„¸ìš”:
1. ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ” ì •ë³´ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
2. ìˆ˜ì¹˜ë‚˜ ë°ì´í„°ë¥¼ ì¸ìš©í•  ë•ŒëŠ” ì •í™•í•˜ê²Œ ì¸ìš©í•˜ì„¸ìš”.
3. ì°¨íŠ¸ë‚˜ ê·¸ë˜í”„ ë¶„ì„ ë‚´ìš©ì´ í¬í•¨ë˜ì–´ ìˆë‹¤ë©´ ê·¸ ë‚´ìš©ë„ í™œìš©í•˜ì„¸ìš”.
4. ì¶œì²˜ë‚˜ ì°¸ê³  ë¬¸ì„œëŠ” ë³„ë„ë¡œ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”. UIì—ì„œ ìë™ìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤.

## ì •ë³´ ë¶€ì¡± ì‹œ ëŒ€ì‘
- ì§ˆë¬¸ì— ëŒ€í•œ **ì§ì ‘ì ì¸ ë‹µë³€**ì´ ì»¨í…ìŠ¤íŠ¸ì— ì—†ìœ¼ë©´:
  1. "ì œê³µëœ ë¬¸ì„œì—ì„œ [ì§ˆë¬¸ ì£¼ì œ]ì— ëŒ€í•œ ì§ì ‘ì ì¸ ì •ë³´ëŠ” ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ëª…ì‹œ
  2. ë‹¨, ê´€ë ¨ëœ ë‚´ìš©ì´ ì¡°ê¸ˆì´ë¼ë„ ìˆë‹¤ë©´ "ë‹¤ë§Œ, ê´€ë ¨í•˜ì—¬ ë‹¤ìŒ ì •ë³´ê°€ ìˆìŠµë‹ˆë‹¤:"ë¡œ ì‹œì‘í•˜ì—¬ ê°„ëµíˆ ì•ˆë‚´
  3. ì™„ì „íˆ ê´€ë ¨ ì—†ëŠ” ë‚´ìš©ë§Œ ìˆë‹¤ë©´ ê·¸ëƒ¥ ì •ë³´ ì—†ìŒë§Œ ì•ˆë‚´
- ë¶€ë¶„ì ì¸ ì •ë³´ë§Œ ìˆëŠ” ê²½ìš°: ìˆëŠ” ì •ë³´ëŠ” ë‹µë³€í•˜ê³ , ì—†ëŠ” ë¶€ë¶„ì€ ëª…í™•íˆ êµ¬ë¶„

ë‹µë³€ì€ ëª…í™•í•˜ê³  êµ¬ì¡°í™”ë˜ì–´ì•¼ í•˜ë©°, í•„ìš”ì‹œ ë¶ˆë¦¿ í¬ì¸íŠ¸ë‚˜ ë²ˆí˜¸ ë§¤ê¸°ê¸°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."""

        messages = [{"role": "system", "content": system_prompt}]

        if conversation_history:
            for msg in conversation_history[-6:]:
                messages.append({"role": msg.role, "content": msg.content})

        user_message = f"""ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

## ì»¨í…ìŠ¤íŠ¸
{context}

## ì§ˆë¬¸
{query}"""

        messages.append({"role": "user", "content": user_message})

        headers = {
            "Authorization": f"Bearer {self.settings.openai_api_key}",
            "Content-Type": "application/json",
        }

        payload = {
            "model": self.settings.llm_model,
            "messages": messages,
            "temperature": 0.3,
            "max_completion_tokens": 2000,
        }

        async with httpx.AsyncClient(timeout=120.0) as client:
            response = await client.post(
                "https://api.openai.com/v1/chat/completions",
                headers=headers,
                json=payload,
            )

        if response.status_code != 200:
            raise Exception(f"OpenAI API ì˜¤ë¥˜: {response.status_code} - {response.text}")

        result = response.json()
        return result["choices"][0]["message"]["content"]

    async def chat_stream(
        self,
        query: str,
        conversation_history: Optional[list[ChatMessage]] = None,
    ) -> AsyncIterator[str]:
        """
        ìŠ¤íŠ¸ë¦¬ë° RAG ì±„íŒ…

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            conversation_history: ì´ì „ ëŒ€í™” ê¸°ë¡

        Yields:
            ì‘ë‹µ í…ìŠ¤íŠ¸ ì²­í¬
        """
        if not self._initialized:
            await self.initialize()

        async for chunk in self._chain.chat_stream(
            query=query,
            conversation_history=conversation_history,
        ):
            yield chunk

    async def search(
        self,
        query: str,
        top_k: int = 5,
        filter_source: Optional[str] = None,
    ) -> RetrievalResult:
        """
        ë¬¸ì„œ ê²€ìƒ‰ë§Œ ìˆ˜í–‰ (LLM ì‘ë‹µ ì—†ì´)

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            filter_source: íŠ¹ì • ì†ŒìŠ¤ë§Œ ê²€ìƒ‰

        Returns:
            ê²€ìƒ‰ ê²°ê³¼
        """
        if not self._initialized:
            await self.initialize()

        return await self._retriever.retrieve(
            query=query,
            config=RetrievalConfig(top_k=top_k),
            filter_source=filter_source,
        )

    async def delete_document(self, source: str) -> None:
        """
        íŠ¹ì • ë¬¸ì„œì˜ ëª¨ë“  ì²­í¬ ì‚­ì œ

        Args:
            source: ì‚­ì œí•  ë¬¸ì„œëª…
        """
        if not self._initialized:
            await self.initialize()

        await self._vector_store.delete_by_source(source)
        print(f"âœ“ ë¬¸ì„œ ì‚­ì œ ì™„ë£Œ: {source}")

    async def close(self) -> None:
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self._vector_store:
            await self._vector_store.close()
        self._initialized = False


async def create_rag_system(
    settings: Optional[Settings] = None,
) -> MultimodalHierarchicalRAG:
    """
    RAG ì‹œìŠ¤í…œ íŒ©í† ë¦¬ í•¨ìˆ˜

    Args:
        settings: ì„¤ì • ê°ì²´

    Returns:
        ì´ˆê¸°í™”ëœ RAG ì‹œìŠ¤í…œ
    """
    rag = MultimodalHierarchicalRAG(settings=settings)
    await rag.initialize()
    return rag
