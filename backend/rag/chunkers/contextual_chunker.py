# -*- coding: utf-8 -*-
"""
V7.5 Contextual Retrieval ëª¨ë“ˆ

Anthropicì˜ Contextual Retrieval ë°©ì‹ êµ¬í˜„:
- ê° ì²­í¬ì— ë¬¸ë§¥ ì •ë³´(ì¶œì²˜, ì„¹ì…˜, ìš”ì•½)ë¥¼ ì¶”ê°€
- ê²€ìƒ‰ ì‹¤íŒ¨ìœ¨ 49% ê°ì†Œ íš¨ê³¼ (Anthropic ë²¤ì¹˜ë§ˆí¬)

Reference: https://www.anthropic.com/news/contextual-retrieval
"""

import os
import asyncio
from dataclasses import dataclass, field
from typing import Optional
import httpx

from rag.parsers import ParsedDocument, ParsedElement, ElementType, CaptionResult
from rag.chunkers.hierarchical_chunker import (
    HierarchicalChunker,
    ParentChunk,
    ChildChunk,
    ChunkRelation,
)


@dataclass
class ContextualChunk:
    """ë¬¸ë§¥ ì •ë³´ê°€ ì¶”ê°€ëœ ì²­í¬"""
    original_content: str
    context_header: str
    contextualized_content: str
    chunk_id: str
    source: str
    page: int
    section: str = ""
    element_type: str = "paragraph"


class ContextGenerator:
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ ì²­í¬ë³„ ë¬¸ë§¥ ì •ë³´ ìƒì„±

    ê° ì²­í¬ì— 50-100 í† í°ì˜ ì„¤ëª…ì  ë¬¸ë§¥ì„ ì¶”ê°€í•˜ì—¬
    ê²€ìƒ‰ ì‹œ í•´ë‹¹ ì²­í¬ê°€ ì–´ë–¤ ë§¥ë½ì—ì„œ ë‚˜ì˜¨ ê²ƒì¸ì§€ ì•Œ ìˆ˜ ìˆê²Œ í•¨
    """

    def __init__(
        self,
        api_key: Optional[str] = None,
        model: str = "gpt-5.2",  # ì¿¼ë¦¬ ë¶„ì„ í’ˆì§ˆ í–¥ìƒ
    ):
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        self.model = model
        self.api_url = "https://api.openai.com/v1/chat/completions"

    async def generate_context(
        self,
        chunk_content: str,
        document_name: str,
        document_summary: str = "",
        section_title: str = "",
        page: int = 0,
    ) -> str:
        """
        ì²­í¬ì— ëŒ€í•œ ë¬¸ë§¥ ì„¤ëª… ìƒì„±

        Args:
            chunk_content: ì²­í¬ ë‚´ìš©
            document_name: ë¬¸ì„œ ì´ë¦„
            document_summary: ë¬¸ì„œ ì „ì²´ ìš”ì•½ (ì„ íƒ)
            section_title: ì„¹ì…˜ ì œëª© (ì„ íƒ)
            page: í˜ì´ì§€ ë²ˆí˜¸

        Returns:
            ë¬¸ë§¥ ì„¤ëª… (50-100 í† í°)
        """
        prompt = f"""<document>
ë¬¸ì„œëª…: {document_name}
{f'ë¬¸ì„œ ìš”ì•½: {document_summary}' if document_summary else ''}
{f'ì„¹ì…˜: {section_title}' if section_title else ''}
í˜ì´ì§€: {page}
</document>

ì•„ë˜ ì²­í¬ì˜ ë‚´ìš©ì„ ë¬¸ì„œ ì „ì²´ ë§¥ë½ì—ì„œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ê°„ê²°í•œ ë¬¸ë§¥ ì„¤ëª…ì„ ì‘ì„±í•˜ì„¸ìš”.
ë¬¸ë§¥ ì„¤ëª…ì€ ì²­í¬ ì•ì— ì¶”ê°€ë˜ì–´ ê²€ìƒ‰ ì‹œ í•´ë‹¹ ì²­í¬ê°€ ë¬´ì—‡ì— ê´€í•œ ê²ƒì¸ì§€ ëª…í™•íˆ ì•Œ ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.

<chunk>
{chunk_content[:1500]}
</chunk>

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ 50-100ìì˜ ë¬¸ë§¥ ì„¤ëª…ë§Œ ì‘ì„±í•˜ì„¸ìš”:
"ì´ ì²­í¬ëŠ” [ë¬¸ì„œëª…]ì˜ [ì„¹ì…˜/ì£¼ì œ]ì—ì„œ [í•µì‹¬ ë‚´ìš©]ì— ëŒ€í•´ ì„¤ëª…í•©ë‹ˆë‹¤."

ë¬¸ë§¥ ì„¤ëª…:"""

        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }

        payload = {
            "model": self.model,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0,
            "max_completion_tokens": 150,
        }

        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(self.api_url, headers=headers, json=payload)

            if response.status_code != 200:
                return self._create_fallback_context(document_name, section_title, page)

            result = response.json()
            context = result["choices"][0]["message"]["content"].strip()
            return context

        except Exception as e:
            print(f"ë¬¸ë§¥ ìƒì„± ì‹¤íŒ¨: {e}")
            return self._create_fallback_context(document_name, section_title, page)

    def _create_fallback_context(
        self,
        document_name: str,
        section_title: str,
        page: int,
    ) -> str:
        """LLM ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë¬¸ë§¥ ìƒì„±"""
        parts = [f"[ì¶œì²˜: {document_name}"]
        if section_title:
            parts.append(f", ì„¹ì…˜: {section_title}")
        if page > 0:
            parts.append(f", í˜ì´ì§€: {page}")
        parts.append("]")
        return "".join(parts)

    async def generate_contexts_batch(
        self,
        chunks: list[dict],
        document_name: str,
        document_summary: str = "",
        batch_size: int = 5,
    ) -> list[str]:
        """
        ì—¬ëŸ¬ ì²­í¬ì— ëŒ€í•´ ë°°ì¹˜ë¡œ ë¬¸ë§¥ ìƒì„±

        Args:
            chunks: ì²­í¬ ì •ë³´ ë¦¬ìŠ¤íŠ¸ [{content, section, page}, ...]
            document_name: ë¬¸ì„œ ì´ë¦„
            document_summary: ë¬¸ì„œ ìš”ì•½
            batch_size: ë™ì‹œ ì²˜ë¦¬ ìˆ˜

        Returns:
            ë¬¸ë§¥ ì„¤ëª… ë¦¬ìŠ¤íŠ¸
        """
        contexts = []

        for i in range(0, len(chunks), batch_size):
            batch = chunks[i:i + batch_size]
            tasks = [
                self.generate_context(
                    chunk_content=c.get("content", ""),
                    document_name=document_name,
                    document_summary=document_summary,
                    section_title=c.get("section", ""),
                    page=c.get("page", 0),
                )
                for c in batch
            ]
            batch_contexts = await asyncio.gather(*tasks)
            contexts.extend(batch_contexts)

            # Rate limiting
            if i + batch_size < len(chunks):
                await asyncio.sleep(0.5)

        return contexts


class ContextualHierarchicalChunker(HierarchicalChunker):
    """
    V7.5 Contextual Hierarchical Chunker

    ê¸°ì¡´ HierarchicalChunkerë¥¼ í™•ì¥í•˜ì—¬ ê° ì²­í¬ì— ë¬¸ë§¥ ì •ë³´ ì¶”ê°€
    """

    def __init__(
        self,
        parent_chunk_size: int = 2000,
        child_chunk_size: int = 500,
        chunk_overlap: int = 50,
        api_key: Optional[str] = None,
        use_llm_context: bool = True,  # LLM ë¬¸ë§¥ ìƒì„± ì‚¬ìš© ì—¬ë¶€
    ):
        super().__init__(
            parent_chunk_size=parent_chunk_size,
            child_chunk_size=child_chunk_size,
            chunk_overlap=chunk_overlap,
        )
        self.context_generator = ContextGenerator(api_key=api_key)
        self.use_llm_context = use_llm_context

    async def chunk_document_with_context(
        self,
        document: ParsedDocument,
        caption_results: Optional[list[CaptionResult]] = None,
        document_summary: str = "",
    ) -> tuple[list[ParentChunk], list[ChildChunk]]:
        """
        ë¬¸ì„œë¥¼ ì²­í‚¹í•˜ê³  ê° ì²­í¬ì— ë¬¸ë§¥ ì •ë³´ ì¶”ê°€

        Args:
            document: íŒŒì‹±ëœ ë¬¸ì„œ
            caption_results: ì´ë¯¸ì§€/ì°¨íŠ¸ ìº¡ì…˜ ê²°ê³¼
            document_summary: ë¬¸ì„œ ì „ì²´ ìš”ì•½ (ì„ íƒ)

        Returns:
            (ë¶€ëª¨ ì²­í¬ ë¦¬ìŠ¤íŠ¸, ìì‹ ì²­í¬ ë¦¬ìŠ¤íŠ¸) - ë¬¸ë§¥ ì •ë³´ í¬í•¨
        """
        # 1. ê¸°ë³¸ ì²­í‚¹ ìˆ˜í–‰
        parent_chunks, child_chunks = self.chunk_document(
            document=document,
            caption_results=caption_results,
        )

        # 2. ë¬¸ë§¥ ì •ë³´ ì¶”ê°€
        if self.use_llm_context:
            child_chunks = await self._add_llm_context(
                child_chunks=child_chunks,
                document_name=document.source,
                document_summary=document_summary,
            )
        else:
            child_chunks = self._add_simple_context(
                child_chunks=child_chunks,
                document_name=document.source,
            )

        # 3. ë¶€ëª¨ ì²­í¬ë„ ë¬¸ë§¥ ì¶”ê°€
        parent_chunks = self._add_parent_context(
            parent_chunks=parent_chunks,
            document_name=document.source,
        )

        return parent_chunks, child_chunks

    async def _add_llm_context(
        self,
        child_chunks: list[ChildChunk],
        document_name: str,
        document_summary: str = "",
    ) -> list[ChildChunk]:
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ê° ì²­í¬ì— ë¬¸ë§¥ ì¶”ê°€"""
        print(f"  ğŸ“ LLM ë¬¸ë§¥ ìƒì„± ì¤‘: {len(child_chunks)}ê°œ ì²­í¬...")

        # ì²­í¬ ì •ë³´ ì¤€ë¹„
        chunk_infos = [
            {
                "content": chunk.content,
                "section": chunk.heading or "",
                "page": chunk.page,
            }
            for chunk in child_chunks
        ]

        # ë°°ì¹˜ë¡œ ë¬¸ë§¥ ìƒì„±
        contexts = await self.context_generator.generate_contexts_batch(
            chunks=chunk_infos,
            document_name=document_name,
            document_summary=document_summary,
            batch_size=10,
        )

        # ë¬¸ë§¥ ì¶”ê°€
        for chunk, context in zip(child_chunks, contexts):
            chunk.content = f"{context}\n\n{chunk.content}"

        print(f"  âœ“ ë¬¸ë§¥ ìƒì„± ì™„ë£Œ")
        return child_chunks

    def _add_simple_context(
        self,
        child_chunks: list[ChildChunk],
        document_name: str,
    ) -> list[ChildChunk]:
        """ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ ë¬¸ë§¥ ì¶”ê°€ (LLM ì—†ì´)"""
        for chunk in child_chunks:
            context_parts = [f"[ì¶œì²˜: {document_name}"]

            if chunk.heading:
                context_parts.append(f", ì„¹ì…˜: {chunk.heading}")

            if chunk.page > 0:
                context_parts.append(f", í˜ì´ì§€: {chunk.page}")

            # ìš”ì†Œ íƒ€ì… ì¶”ê°€
            if hasattr(chunk, 'element_type') and chunk.element_type:
                type_map = {
                    "table": "í…Œì´ë¸”",
                    "image": "ì´ë¯¸ì§€/ì°¨íŠ¸",
                    "chart": "ì°¨íŠ¸",
                    "paragraph": "ë³¸ë¬¸",
                }
                element_type_kr = type_map.get(chunk.element_type, chunk.element_type)
                context_parts.append(f", ìœ í˜•: {element_type_kr}")

            context_parts.append("]")
            context_header = "".join(context_parts)

            chunk.content = f"{context_header}\n\n{chunk.content}"

        return child_chunks

    def _add_parent_context(
        self,
        parent_chunks: list[ParentChunk],
        document_name: str,
    ) -> list[ParentChunk]:
        """ë¶€ëª¨ ì²­í¬ì— ë¬¸ë§¥ ì¶”ê°€"""
        for chunk in parent_chunks:
            context_parts = [f"[ë¬¸ì„œ: {document_name}"]

            if chunk.heading:
                context_parts.append(f", ì„¹ì…˜: {chunk.heading}")

            if chunk.start_page > 0:
                if chunk.start_page == chunk.end_page:
                    context_parts.append(f", í˜ì´ì§€: {chunk.start_page}")
                else:
                    context_parts.append(f", í˜ì´ì§€: {chunk.start_page}-{chunk.end_page}")

            context_parts.append("]")
            context_header = "".join(context_parts)

            chunk.content = f"{context_header}\n\n{chunk.content}"

        return parent_chunks


async def generate_document_summary(
    document: ParsedDocument,
    api_key: Optional[str] = None,
    model: str = "gpt-5.2",
) -> str:
    """
    ë¬¸ì„œ ì „ì²´ ìš”ì•½ ìƒì„± (ì„ íƒì  ì‚¬ìš©)

    Args:
        document: íŒŒì‹±ëœ ë¬¸ì„œ
        api_key: OpenAI API í‚¤
        model: ì‚¬ìš©í•  ëª¨ë¸

    Returns:
        ë¬¸ì„œ ìš”ì•½ (200-300ì)
    """
    api_key = api_key or os.getenv("OPENAI_API_KEY")

    # ë¬¸ì„œì˜ ì²˜ìŒ ë¶€ë¶„ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    text_parts = []
    for element in document.elements[:20]:  # ì²˜ìŒ 20ê°œ ìš”ì†Œ
        if element.element_type in (ElementType.PARAGRAPH, ElementType.HEADING):
            text_parts.append(element.content)

    sample_text = "\n".join(text_parts)[:3000]

    prompt = f"""ë‹¤ìŒ ë¬¸ì„œì˜ ë‚´ìš©ì„ 200-300ìë¡œ ìš”ì•½í•˜ì„¸ìš”.
ë¬¸ì„œì˜ ì£¼ì œ, ëª©ì , ì£¼ìš” ë‚´ìš©ì„ í¬í•¨í•´ì£¼ì„¸ìš”.

ë¬¸ì„œëª…: {document.source}

ë‚´ìš©:
{sample_text}

ìš”ì•½:"""

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
    }

    payload = {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0,
        "max_completion_tokens": 500,
    }

    try:
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                "https://api.openai.com/v1/chat/completions",
                headers=headers,
                json=payload,
            )

        if response.status_code == 200:
            result = response.json()
            return result["choices"][0]["message"]["content"].strip()
    except Exception as e:
        print(f"ë¬¸ì„œ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")

    return ""
